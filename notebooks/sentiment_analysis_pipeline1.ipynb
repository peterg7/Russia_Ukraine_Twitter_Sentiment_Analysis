{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Pipeline #1\n",
    "*Refer to `notebooks/README.md` for an explanation of the various pipelines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import re\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "\n",
    "# Graphing/Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Prep nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For JupyterThemes ##\n",
    "\n",
    "# Set plot style\n",
    "# from jupyterthemes import jtplot\n",
    "# jtplot.style(theme='onedork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_AND_PUNCT_PATTERN = r'https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*'\n",
    "CURRENCY_PATTERN = r'\\$[a-zA-Z0-9]*'\n",
    "NON_ALPHA_NUM_PATTERN = r'[^a-zA-Z\\']'\n",
    "MENTIONS_PATTERN = r'\\@[a-zA-Z0-9]*'\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "def extract(data_path, col_rename_map, target_words):\n",
    "\n",
    "    # Import data\n",
    "    raw_tweets_df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Rename columns\n",
    "    tweets_df = raw_tweets_df[list(col_rename_map.keys())].rename(columns=col_rename_map)\n",
    "\n",
    "    # Drop duplicate tweets\n",
    "    tweets_df = tweets_df.drop_duplicates(subset='tweet', keep='first')\n",
    "\n",
    "    # Initialize Lemmatizer\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    def cleanTweet(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        tweet = re.sub(URL_AND_PUNCT_PATTERN, ' ', tweet)\n",
    "        tweet = re.sub(CURRENCY_PATTERN, ' ', tweet)\n",
    "        tweet = re.sub(MENTIONS_PATTERN, ' ', tweet)\n",
    "        tweet = re.sub(NON_ALPHA_NUM_PATTERN, ' ', tweet)\n",
    "        tweet = ' '.join([w for w in tweet.split() if len(w) > 1])\n",
    "        \n",
    "        trimmed_lemma_words = [lemma.lemmatize(x) for x in nltk.wordpunct_tokenize(tweet) \n",
    "                                    if x not in STOP_WORDS]\n",
    "        clean_tweet = ' '.join(trimmed_lemma_words)\n",
    "        \n",
    "        return [lemma.lemmatize(x, nltk.corpus.reader.wordnet.VERB) \n",
    "                    for x in nltk.wordpunct_tokenize(clean_tweet) if x not in STOP_WORDS]\n",
    "\n",
    "    def cleanHashtags(hashtags):\n",
    "        if hashtags:\n",
    "            hashtags = hashtags.lower()\n",
    "            hashtags = re.sub(CURRENCY_PATTERN, ' ', hashtags)\n",
    "            hashtags = re.sub(NON_ALPHA_NUM_PATTERN, ' ', hashtags)\n",
    "            hashtags = hashtags.strip() \n",
    "        return hashtags\n",
    "    \n",
    "    \n",
    "    # Clean tweets\n",
    "    tweets_df['clean_tweet_words'] = tweets_df['tweet'].apply(lambda x: cleanTweet(x))\n",
    "    tweets_df['clean_tweet'] = tweets_df['clean_tweet_words'].apply(lambda x:' '.join(x))\n",
    "\n",
    "    # Clean hashtags\n",
    "    tweets_df[\"hashtags\"] = tweets_df[\"hashtags\"].astype(str)\n",
    "    tweets_df[\"hashtags\"] = tweets_df[\"hashtags\"].apply(lambda x: cleanHashtags(x))\n",
    "\n",
    "    # Convert date to datetime and extract month/year\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "    tweets_df['month'] = tweets_df['date'].dt.month\n",
    "    tweets_df['year'] = tweets_df['date'].dt.year\n",
    "\n",
    "    # Remove all tweets which do not have the provided target words\n",
    "    keywords_str = '|'.join(target_words)\n",
    "    target_tweets_df = tweets_df.copy()\n",
    "    target_tweets_df = target_tweets_df[target_tweets_df[\"clean_tweet\"].str.contains(keywords_str)]\n",
    "\n",
    "    return target_tweets_df, tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_THRESHOLD = 0.15\n",
    "INIT_WORD_VEC_ALPHA = 0.03\n",
    "MIN_WORD_VEC_ALPHA = 0.007\n",
    "WORD_VEC_SIZE = 300\n",
    "WORD_VEC_SAMPLE = 1e-5\n",
    "MAX_PRED_DISTANCE = 5\n",
    "MIN_WORD_FREQ = 4\n",
    "NEGATIVE_SAMPLING_VALUE = 20\n",
    "\n",
    "def transform(filtered_df, cumulative_df, load_word_vec='', save_word_vec='', \n",
    "                load_embeddings='', save_embeddings=''):\n",
    "    \n",
    "    # Preseve original df\n",
    "    transform_tweets_df = filtered_df.copy()\n",
    "    \n",
    "    if load_word_vec:\n",
    "        # Load existing vectors if path provided\n",
    "        word_vectors = Word2Vec.load(load_word_vec).wv\n",
    "\n",
    "    else:\n",
    "        # Restructure the `clean_text` column\n",
    "        row_sentences = [row for row in cumulative_df[\"clean_tweet_words\"]]\n",
    "\n",
    "        # Detect common phrases (bigrams) from a list of sentences\n",
    "        phrases = Phrases(row_sentences, min_count=1, progress_per=50000)\n",
    "        bigram = Phraser(phrases)\n",
    "        sentences = bigram[row_sentences]\n",
    "        \n",
    "        # Initialize vector model\n",
    "        word_vec_model = Word2Vec(vector_size=WORD_VEC_SIZE, window=MAX_PRED_DISTANCE, \n",
    "                                    min_count=MIN_WORD_FREQ, workers=multiprocessing.cpu_count()-1,\n",
    "                                    negative=NEGATIVE_SAMPLING_VALUE, sample=WORD_VEC_SAMPLE, \n",
    "                                    alpha=INIT_WORD_VEC_ALPHA, min_alpha=MIN_WORD_VEC_ALPHA,  \n",
    "                                    seed= 42)\n",
    "\n",
    "        # Establish dataset for the vector model\n",
    "        word_vec_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "        # Train the model\n",
    "        word_vec_model.train(sentences, \n",
    "                    total_examples=word_vec_model.corpus_count, \n",
    "                    epochs=30, \n",
    "                    report_delay=1)\n",
    "        \n",
    "        if save_word_vec:\n",
    "            # Store current word vector model\n",
    "            word_vec_model.save(save_word_vec)\n",
    "\n",
    "        word_vectors = word_vec_model.wv\n",
    "    \n",
    "    # Build KMeans model to cluster words into positive, negative, and neutral clusters\n",
    "    cluster_model = KMeans(n_clusters=3, \n",
    "               max_iter=1000, \n",
    "               random_state=42, \n",
    "               n_init=50).fit(X=word_vectors.vectors.astype('double'))\n",
    "                        \n",
    "\n",
    "    ## Need user input to determine each cluster's sentiment ##\n",
    "    print('** Top 10 Similar Word Vectors By Cluster**')\n",
    "    print('Cluster 0:')\n",
    "    print(word_vectors.similar_by_vector(cluster_model.cluster_centers_[0], \n",
    "                                            topn=10, restrict_vocab=None))\n",
    "    print('\\nCluster 1:')\n",
    "    print(word_vectors.similar_by_vector(cluster_model.cluster_centers_[1], \n",
    "                                            topn=10, restrict_vocab=None))\n",
    "    print('\\nCluster 2:')\n",
    "    print(word_vectors.similar_by_vector(cluster_model.cluster_centers_[2], \n",
    "                                            topn=10, restrict_vocab=None))\n",
    "    \n",
    "    print('Label each cluster: -1 = positive, 0 = neutral, 1 = positive')\n",
    "    cluster_sentiment_defs = []\n",
    "    user_input = ''\n",
    "    while len(cluster_sentiment_defs) < 3 and user_input != 'q':\n",
    "        user_input = input(f'Cluster {len(cluster_sentiment_defs)} value:')\n",
    "        if user_input == 'q':\n",
    "            return\n",
    "        try:\n",
    "            value = int(user_input)\n",
    "            if value in cluster_sentiment_defs or value not in range(-1, 1):\n",
    "                print('Already used this sentiment or not in range (-1, 0, 1)')\n",
    "                continue\n",
    "            cluster_sentiment_defs.append(value)\n",
    "        except ValueError:\n",
    "            print('Need a number in range (-1, 0, 1). Press q to exit')\n",
    "\n",
    "    # Assign 1 to positive values, 0 to neutral and -1 for negative values\n",
    "    # cluster_sentiment_map = defaultdict(lambda : -1, { 2: 1, 0: 0 })\n",
    "\n",
    "    # Mapping for sentiment encodings\n",
    "    sentiment_mapping = { 0: \"neutral\", 1: \"positive\", -1: \"negative\" }\n",
    "\n",
    "    if load_embeddings:\n",
    "        # Load existing embeddings file\n",
    "        words_df = pd.read_csv(load_embeddings)\n",
    "\n",
    "    else:\n",
    "        # Create a DataFrame of words with their embeddings and cluster values\n",
    "        words_df = pd.DataFrame(word_vectors.index_to_key)\n",
    "        words_df.columns = ['words']\n",
    "        words_df['vectors'] = words_df.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "        words_df['cluster'] = words_df.vectors.apply(lambda x: cluster_model.predict([np.array(x)]))\n",
    "        words_df.cluster = words_df.cluster.apply(lambda x: x[0])\n",
    "\n",
    "        \n",
    "        words_df['cluster_value'] = [cluster_sentiment_defs[i] for i in words_df.cluster]\n",
    "        # words_df['cluster_value'] = [1 if i==2 else 0 if i==0 else -1 for i in words_df.cluster]\n",
    "\n",
    "        # Calculate proximity of words in each vector\n",
    "        calc_vector_nearness = lambda x: 1 / (cluster_model.transform([x.vectors]).min())\n",
    "        words_df['closeness_score'] = words_df.apply(calc_vector_nearness, axis=1)\n",
    "        words_df['sentiment_coeff'] = words_df.closeness_score * words_df.cluster_value\n",
    "\n",
    "        # Map sentiment encodings\n",
    "        words_df[\"sentiments\"] = words_df[\"cluster_value\"].map(sentiment_mapping)\n",
    "\n",
    "        if save_embeddings:\n",
    "            # Store current word embeddings\n",
    "            words_df.to_csv(save_embeddings)\n",
    "\n",
    "    # Get the sentiment for the entire tweet\n",
    "    def getSentiment(row, sentiment_dict):\n",
    "        total, count = 0, 0\n",
    "        test = row[\"clean_tweet_words\"]\n",
    "        for t in test:\n",
    "            total += int(words_cluster_dict.get(t, 0))\n",
    "            # if score := words_cluster_dict.get(t):\n",
    "            #     total += int(score)\n",
    "            count += 1 \n",
    "            \n",
    "        avg = total / count\n",
    "        return -1 if (avg < -SENTIMENT_THRESHOLD) else 1 if (avg > SENTIMENT_THRESHOLD) else 0\n",
    "\n",
    "\n",
    "    # Create a dictionary of the word and its cluster value\n",
    "    words_cluster_dict = dict(zip(words_df.words, words_df.cluster_value))\n",
    "\n",
    "    # Add sentiment column (integer values)\n",
    "    transform_tweets_df[\"sentiment\"] = transform_tweets_df.apply(getSentiment,\n",
    "                                                         args=(words_cluster_dict,),\n",
    "                                                         axis=1)\n",
    "\n",
    "    # Map integer sentiment to word value\n",
    "    transform_tweets_df[\"sentiments_val\"] = transform_tweets_df[\"sentiment\"].map(sentiment_mapping)\n",
    "\n",
    "\n",
    "    return transform_tweets_df, words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "DFLT_VECTOR_OUT_PATTERN = r'\\/[^/]*$'\n",
    "\n",
    "def model(sent_analyzed_df, load_svc='', save_svc='', load_nb='', save_nb='', \n",
    "                load_vectorizer='', save_vectorizer=''):\n",
    "    \n",
    "    # Convert each sentiment to df (no need to worry about memory crash, small dataset)\n",
    "    pos_df = sent_analyzed_df[sent_analyzed_df[\"sentiments_val\"]==\"positive\"]\n",
    "    neg_df = sent_analyzed_df[sent_analyzed_df[\"sentiments_val\"]==\"negative\"]\n",
    "    neu_df = sent_analyzed_df[sent_analyzed_df[\"sentiments_val\"]==\"neutral\"]\n",
    "\n",
    "    # Combine all sentiments in one df\n",
    "    sentiments_df_list = [pos_df, neg_df, neu_df] \n",
    "    agg_sentiment_df = pd.concat(sentiments_df_list)\n",
    "\n",
    "    # Split the data to training, testing, and validation data \n",
    "    train_test_df, _ = train_test_split(agg_sentiment_df, test_size=TEST_SIZE, random_state=10)\n",
    "\n",
    "    X = train_test_df['clean_tweet']\n",
    "    y = train_test_df['sentiment']\n",
    "\n",
    "    # Split the dataset set int0 training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "    # Instantiate TfidfVectorizer \n",
    "    if load_vectorizer:\n",
    "        vectorizer = load(load_vectorizer)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(min_df=3,\n",
    "                                    sublinear_tf=True,\n",
    "                                    ngram_range=(1,2),\n",
    "                                    stop_words='english')\n",
    "\n",
    "    # Fit vectorizer\n",
    "    X_train_tf = vectorizer.fit_transform(X_train.reset_index()[\"clean_tweet\"]).toarray()\n",
    "    X_test_tf = vectorizer.transform(X_test.reset_index()[\"clean_tweet\"]).toarray()\n",
    "\n",
    "    # Store features from the vectors\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a dict of Sentiment_val: sentiments to use with the confusion matrix\n",
    "    sentiment_id_df = agg_sentiment_df[['sentiments_val', 'sentiment']].drop_duplicates() \\\n",
    "                                                                        .sort_values('sentiment')\n",
    "    sentiment_to_id = dict(sentiment_id_df.values)\n",
    "\n",
    "    ## LinearSVC ##\n",
    "\n",
    "    if load_svc:\n",
    "        # Import previous linearSVC model (NOTE: Must use same vectorizer from its fitting)\n",
    "        linearSVC = load(load_svc)\n",
    "\n",
    "    else:\n",
    "        # Instantiate the model\n",
    "        linearSVC = LinearSVC(random_state=0)\n",
    "\n",
    "        # Fit the model\n",
    "        linearSVC.fit(X_train_tf, y_train)\n",
    "\n",
    "        if save_svc:\n",
    "            dump(linearSVC, save_svc)\n",
    "\n",
    "    # Predict\n",
    "    svc_y_pred = linearSVC.predict(X_test_tf)\n",
    "\n",
    "    # Build confusion matrix to evaluate the model results\n",
    "    svc_conf_mat = confusion_matrix(y_test, svc_y_pred, labels=np.unique(svc_y_pred))\n",
    "\n",
    "    # Get classification report\n",
    "    svc_classification = classification_report(y_test, svc_y_pred, labels=np.unique(svc_y_pred))\n",
    "\n",
    "    # Use score method to get accuracy of model\n",
    "    svc_score = linearSVC.score(X_test_tf, y_test)\n",
    "\n",
    "    ## MultinomialNB ##\n",
    "\n",
    "    if load_nb:\n",
    "        # Import previous multinomialNB model (NOTE: Must use same vectorizer from its fitting)\n",
    "        multiNB = load(load_nb)\n",
    "\n",
    "    else:\n",
    "        # Instantiate the model\n",
    "        multiNB = MultinomialNB()\n",
    "\n",
    "        # Fit the model\n",
    "        multiNB.fit(X_train_tf, y_train)\n",
    "\n",
    "        if save_nb:\n",
    "            dump(multiNB, save_nb)\n",
    "\n",
    "    # Predict\n",
    "    nb_y_pred = multiNB.predict(X_test_tf)\n",
    "\n",
    "    # Build confusion matrix to evaluate the model results\n",
    "    nb_conf_mat = confusion_matrix(y_test, nb_y_pred, labels=np.unique(nb_y_pred))\n",
    "\n",
    "    # Get classification report\n",
    "    nb_classification = classification_report(y_test, nb_y_pred, labels=np.unique(nb_y_pred))\n",
    "\n",
    "    # Use score method to get accuracy of model\n",
    "    nb_score = multiNB.score(X_test_tf, y_test)\n",
    "\n",
    "    if save_vectorizer:\n",
    "        dump(vectorizer, save_vectorizer)\n",
    "\n",
    "    elif save_svc or save_nb:\n",
    "        vector_out_match = re.search(DFLT_VECTOR_OUT_PATTERN, save_svc if save_svc else save_nb)\n",
    "        if not vector_out_match.group():\n",
    "            print('Error: could not extract default vectorizer output path')\n",
    "        else:\n",
    "            dump(vectorizer, re.sub(vector_out_match.group(), '/vectorizer.joblib', \n",
    "                    vector_out_match.string))\n",
    "\n",
    "    return {\n",
    "        'LinearSVC': {\n",
    "            'model': linearSVC,\n",
    "            'conf_mat': svc_conf_mat,\n",
    "            'classification': svc_classification,\n",
    "            'score': svc_score,\n",
    "        },\n",
    "        'MultinomialNB': {\n",
    "            'model': multiNB,\n",
    "            'conf_mat': nb_conf_mat,\n",
    "            'classification': nb_classification,\n",
    "            'score': nb_score,\n",
    "        },\n",
    "        'features': feature_names,\n",
    "        'vectorizer': vectorizer,\n",
    "        'sentiment_id': sentiment_id_df,\n",
    "        'sentiment_to': sentiment_to_id\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(data_path, column_mappings, target_words, build_models=False, io_options={}):\n",
    "\n",
    "    print('Initializing Stage 1: Extracting...')\n",
    "\n",
    "    target_df, all_tweets_df = extract(data_path=data_path, \n",
    "                                col_rename_map=column_mappings,\n",
    "                                target_words=target_words)\n",
    "\n",
    "    print('Completed Stage 1.\\n\\nInitializing Stage 2: Transforming...')\n",
    "\n",
    "    result = transform(filtered_df=target_df, \n",
    "                        cumulative_df=all_tweets_df,\n",
    "                        load_word_vec=io_options.get('load_word_vec', ''),\n",
    "                        save_word_vec=io_options.get('save_word_vec', ''),\n",
    "                        load_embeddings=io_options.get('load_embedding', ''),\n",
    "                        save_embeddings=io_options.get('save_embeddings', ''))\n",
    "\n",
    "    transform_tweets_df, word_vecs = result if result else None, None\n",
    "\n",
    "    print('Completed Stage 2.')\n",
    "\n",
    "    model_dict = None\n",
    "    if build_models:\n",
    "        print('Initializing Stage 3: Modeling...')\n",
    "        model_dict = model(sent_analyzed_df=transform_tweets_df,\n",
    "                            load_svc=io_options.get('load_svc', ''),\n",
    "                            save_svc=io_options.get('save_svc', ''),\n",
    "                            load_nb=io_options.get('load_nb', ''),\n",
    "                            save_nb=io_options.get('save_nb', ''),\n",
    "                            load_vectorizer=io_options.get('load_vectorizer', ''),\n",
    "                            save_vectorizer=io_options.get('save_vectorizer', ''))\n",
    "        print('Completed Stage 3.')\n",
    "\n",
    "    return transform_tweets_df, word_vecs, model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Top 10 Similar Word Vectors By Cluster**\n",
      "Cluster 0:\n",
      "[('leak', 0.9998458623886108), ('warn', 0.9998443126678467), ('mor', 0.9998430609703064), ('thousand', 0.9998424649238586), ('ball', 0.9998416900634766), ('face', 0.9998406767845154), ('priceless', 0.9998401403427124), ('street', 0.9998401403427124), ('abc_news', 0.9998400211334229), ('johnsonout_hoyleout', 0.9998390674591064)]\n",
      "\n",
      "Cluster 1:\n",
      "[('warn', 0.9998500943183899), ('leak', 0.9998488426208496), ('ball', 0.9998477697372437), ('southern', 0.9998451471328735), ('heartbreaking', 0.999844491481781), ('street', 0.9998442530632019), ('thousand', 0.9998438954353333), ('minute', 0.9998419880867004), ('oligarch', 0.9998415112495422), ('face', 0.9998412728309631)]\n",
      "\n",
      "Cluster 2:\n",
      "[('warn', 0.9998506903648376), ('leak', 0.9998481273651123), ('street', 0.9998452067375183), ('ball', 0.9998452067375183), ('southern', 0.9998446106910706), ('thousand', 0.9998442530632019), ('heartbreaking', 0.9998435974121094), ('face', 0.9998415112495422), ('johnsonout_hoyleout', 0.9998413920402527), ('abc_news', 0.9998411536216736)]\n",
      "Label each cluster: -1 = positive, 0 = neutral, 1 = positive\n",
      "Need a number in range (-1, 0, 1). Press q to exit\n",
      "Need a number in range (-1, 0, 1). Press q to exit\n",
      "Need a number in range (-1, 0, 1). Press q to exit\n",
      "Need a number in range (-1, 0, 1). Press q to exit\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=10'>11</a>\u001b[0m keywords \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mukraine\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrussia\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mzelensky\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=11'>12</a>\u001b[0m io_params \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mload_word_vec\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=13'>14</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msave_word_vec\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m../models/word_vec.model\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=21'>22</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msave_vectorizer\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m../models/vectorizer.joblib\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=22'>23</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=24'>25</a>\u001b[0m sentiment_df, word_vecs, model_dict \u001b[39m=\u001b[39m pipeline(data_path\u001b[39m=\u001b[39;49mdata_path,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=25'>26</a>\u001b[0m                                                 column_mappings\u001b[39m=\u001b[39;49mcol_rename_map,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=26'>27</a>\u001b[0m                                                 target_words\u001b[39m=\u001b[39;49mkeywords,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=27'>28</a>\u001b[0m                                                 io_options\u001b[39m=\u001b[39;49m{},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000009?line=28'>29</a>\u001b[0m                                                 build_models\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb Cell 10'\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(data_path, column_mappings, target_words, build_models, io_options)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpipeline\u001b[39m(data_path, column_mappings, target_words, build_models\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, io_options\u001b[39m=\u001b[39m{}):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=2'>3</a>\u001b[0m     target_df, all_tweets_df \u001b[39m=\u001b[39m extract(data_path\u001b[39m=\u001b[39mdata_path, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=3'>4</a>\u001b[0m                                 col_rename_map\u001b[39m=\u001b[39mcolumn_mappings,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=4'>5</a>\u001b[0m                                 target_words\u001b[39m=\u001b[39mtarget_words)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=8'>9</a>\u001b[0m     transform_tweets_df, word_vecs \u001b[39m=\u001b[39m transform(filtered_df\u001b[39m=\u001b[39mtarget_df, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=9'>10</a>\u001b[0m                                                 cumulative_df\u001b[39m=\u001b[39mall_tweets_df,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=10'>11</a>\u001b[0m                                                 load_word_vec\u001b[39m=\u001b[39mio_options\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mload_word_vec\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=11'>12</a>\u001b[0m                                                 save_word_vec\u001b[39m=\u001b[39mio_options\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39msave_word_vec\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=12'>13</a>\u001b[0m                                                 load_embeddings\u001b[39m=\u001b[39mio_options\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mload_embedding\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=13'>14</a>\u001b[0m                                                 save_embeddings\u001b[39m=\u001b[39mio_options\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39msave_embeddings\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=15'>16</a>\u001b[0m     model_dict \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/petergish/Nucleus/School/Undergrad/USF/Computer_Science/Data_Visualization/FinalProject/twitter_sentiment_analysis/notebooks/sentiment_analysis_pipeline1.ipynb#ch0000008?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m build_models:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "data_path = '../data/raw/slava_ukraini_tweets.csv'\n",
    "col_rename_map = {\n",
    "    'date': 'date',\n",
    "    'user_name': 'username',\n",
    "    # 'user_description': 'description',\n",
    "    # 'user_location': 'location',\n",
    "    'retweets': 'retweets',\n",
    "    'text': 'tweet',\n",
    "    'hashtags': 'hashtags'\n",
    "}\n",
    "keywords = ['ukraine', 'russia', 'zelensky']\n",
    "io_params = {\n",
    "    'load_word_vec': '',\n",
    "    'save_word_vec': '../models/word_vec.model',\n",
    "    'load_embeddings': '',\n",
    "    'save_embeddings': '../data/embeddings/words.csv',\n",
    "    'load_svc': '',\n",
    "    'save_svc': '../models/linearSVC.joblib',\n",
    "    'load_nb': '',\n",
    "    'save_nb': '../models/multinomialNB.joblib',\n",
    "    'load_vectorizer': '',\n",
    "    'save_vectorizer': '../models/vectorizer.joblib'\n",
    "}\n",
    "\n",
    "sentiment_df, word_vecs, model_dict = pipeline(data_path=data_path,\n",
    "                                                column_mappings=col_rename_map,\n",
    "                                                target_words=keywords,\n",
    "                                                io_options={},\n",
    "                                                build_models=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.sentiments_val.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphicProcessors:\n",
    "    \n",
    "    # Display a word cloud with the given text\n",
    "    def generateWordcloud(text):\n",
    "        words=' '.join([words for words in text])\n",
    "        wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(words)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Shorthand functions for commonly used plots\n",
    "    graphWordDistribution = lambda word_vecs: GraphicProcessors.graphDistribution(word_vecs, 'sentiments', 'Sentiment Distribution of Words')\n",
    "    graphTweetDistribution = lambda tweet_df: GraphicProcessors.graphDistribution(tweet_df, 'sentiments_val', 'Sentiment Distribution of Tweets')\n",
    "\n",
    "    # Make a pie chart from a dataframe's column distribution\n",
    "    def graphDistribution(df, plot_col, title='', fig_size=(7, 7)):\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(fig_size)\n",
    "        colors = [\"crimson\", \"steelblue\", \"mediumseagreen\"]\n",
    "\n",
    "        pie_df = df[plot_col].value_counts().reset_index()\n",
    "\n",
    "        plt.pie(pie_df[plot_col],\n",
    "                labels=pie_df[\"index\"],\n",
    "                radius=2,\n",
    "                colors=colors,\n",
    "                autopct=\"%1.1f%%\")\n",
    "\n",
    "        plt.axis('equal')\n",
    "        if title:\n",
    "            plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "        return pie_df\n",
    "\n",
    "    # Display a bar chart with the counts of values within a specific column\n",
    "    def graphKeywordCounts(df, plot_col, count_col, keywords):\n",
    "        # Inspect keyword sentiment\n",
    "        pattern = '|'.join(keywords)\n",
    "        keyword_sent_df = df[(df[plot_col].str.contains(pattern))]\n",
    "        sns.countplot(x=keyword_sent_df[count_col]);\n",
    "\n",
    "    # Shorthand functions for commonly used plots\n",
    "    graphTop10Usernames = lambda tweets_df: GraphicProcessors.graphCounts(tweets_df, 'username', 'sentiments_val', 'Top 10 Highest Tweeting usernames', tweets_df['username'].value_counts().iloc[:10].index)\n",
    "    graphTop10Hashtags = lambda tweets_df: GraphicProcessors.graphCounts(tweets_df, 'hashtags', 'sentiments_val', 'Top 10 Hashtags', tweets_df['hashtags'].value_counts().iloc[1:10].index, (15,10))\n",
    "\n",
    "    # Display a more detailed bar chart from `graphKeywordSentiment`\n",
    "    def graphCounts(df, x_col, hue_col=None, title='', order=None, plt_size=(10,8)):\n",
    "        fig = plt.subplots(figsize=plt_size)\n",
    "        if title:\n",
    "            plt.title(title, fontsize=20)\n",
    "        chart = sns.countplot(x=x_col, \n",
    "                                data=df, \n",
    "                                palette=\"Set2\", \n",
    "                                hue=hue_col,\n",
    "                                order=order)\n",
    "\n",
    "        chart.set_xticklabels(chart.get_xticklabels(),\n",
    "                                rotation=30, \n",
    "                                horizontalalignment='right')\n",
    "\n",
    "    # Display a confusion matrix generated by a sklearn/tensorflow model\n",
    "    def graphConfusionmatrix(conf_mat, sentiment_id_df):\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        sns.heatmap(conf_mat, \n",
    "                    annot=True, \n",
    "                    fmt='d',\n",
    "                    xticklabels=sentiment_id_df.sentiments_val.values, \n",
    "                    yticklabels=sentiment_id_df.sentiment.values)\n",
    "\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "\n",
    "    # Graph a bar chart of the top portion of feature coefficients from a model\n",
    "    def graphCoefficients(model, feature_names, top_features=20, fig_size=(15, 5)):\n",
    "\n",
    "        coefficients_and_features = sorted(zip(model.coef_[0], feature_names)) \n",
    "        features_coef_df = pd.DataFrame(coefficients_and_features)\n",
    "        features_coef_df.columns = 'coefficient','word'\n",
    "        features_coef_df.sort_values(by='coefficient')\n",
    "\n",
    "        num_features = len(feature_names)\n",
    "        neg_coefficients = model.coef_[-1][:num_features]\n",
    "        pos_coefficients = model.coef_[1][:num_features]\n",
    "        top_pos_coefficients = np.argsort(pos_coefficients[pos_coefficients > 0])[-top_features:]\n",
    "        top_neg_coefficients = np.argsort(pos_coefficients[neg_coefficients < 0])[:top_features]\n",
    "        top_coefficients = np.hstack([top_neg_coefficients, top_pos_coefficients])\n",
    "        total_coefficients = np.hstack([neg_coefficients, pos_coefficients])\n",
    "        \n",
    "        # create plot\n",
    "        fig = plt.figure(figsize=fig_size)\n",
    "        colors = ['red' if c < 0 else 'blue' for c in total_coefficients[top_coefficients]]\n",
    "        feature_names = np.array(feature_names)\n",
    "\n",
    "        plt.bar(np.arange(2 * top_features), total_coefficients[top_coefficients], color=colors)\n",
    "        plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], rotation=60, ha='right')\n",
    "        title=\"Positive and Negative Labels\"\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphicProcessors.graphWordDistribution(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphicProcessors.graphTweetDistribution(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphicProcessors.graphKeywordCounts(sentiment_df, 'clean_tweets', 'sentimants_val', ['russia'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphicProcessors.graphTop10Usernames(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphicProcessors.graphTop10Hashtags(sentiment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphicProcessors.generateWordcloud(sentiment_df[sentiment_df[\"sentiment\"]==1][\"cleaned_tweet\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphicProcessors.generateWordcloud(sentiment_df[sentiment_df[\"sentiment\"]==-1][\"cleaned_tweet\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphicProcessors.graphConfusionmatrix(model_dict['LinearSVC']['conf_mat'], model_dict['sentiment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphicProcessors.graphCoefficients(model_dict['LinearSVC']['model'], model_dict['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d883baf2d2c020187d16fb74e1bc85e676b385dd78044a08a209b4abcafece"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
