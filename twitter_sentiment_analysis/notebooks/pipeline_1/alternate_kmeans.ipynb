{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "import matplotlib.pyplot as plt \n",
    "from numpy.linalg import norm\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "sns.set()\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prep nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist, euclidean\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_patterns = [\n",
    "            r\"https?:\\\\/\\\\/[a-zA-Z0-9@:%._\\\\/+~#=?&;-]*\",\n",
    "            r\"\\\\$[a-zA-Z0-9]*\",\n",
    "            r\"\\\\@[a-zA-Z0-9]*\",\n",
    "            r\"[^a-zA-Z\\\\\\\"]\"\n",
    "        ]\n",
    "\n",
    "hashtag_patterns = [\n",
    "            r\"\\\\$[a-zA-Z0-9]*\",\n",
    "            r\"[^a-zA-Z\\\\\\\"]\"\n",
    "        ]\n",
    "\n",
    "column_mappings = {\n",
    "            \"date\": \"date\",\n",
    "            \"user_name\": \"username\",\n",
    "            \"retweets\": \"retweets\",\n",
    "            \"text\": \"tweet\",\n",
    "            \"hashtags\": \"hashtags\"\n",
    "        }\n",
    "\n",
    "filter_words = [\n",
    "            \"ukraine\",\n",
    "            \"russia\",\n",
    "            \"zelensky\"\n",
    "        ]\n",
    "\n",
    "sentiment_map = {\n",
    "            -1: \"negative\",\n",
    "            0: \"neutral\",\n",
    "            1: \"positive\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    import_version = 12\n",
    "    import_dest = \"../../data/slava_ukraini_tweets_v{version}.csv\".format(version=import_version)\n",
    "    raw_tweets_df = pd.read_csv(import_dest)\n",
    "\n",
    "\n",
    "    # Rename columns\n",
    "    tweets_df = raw_tweets_df[list(column_mappings.keys())].rename(columns=column_mappings)\n",
    "\n",
    "    # Drop duplicate tweets\n",
    "    tweets_df = tweets_df.drop_duplicates(subset='tweet', keep='first')\n",
    "\n",
    "    # Initialize Lemmatizer\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    def cleanTweet(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        for pattern in tweets_patterns:\n",
    "            tweet = re.sub(pattern, ' ', tweet)\n",
    "        tweet = ' '.join([w for w in tweet.split() if len(w) > 1])\n",
    "        \n",
    "        trimmed_lemma_words = [lemma.lemmatize(x) for x in nltk.wordpunct_tokenize(tweet) \n",
    "                                    if x not in stop_words]\n",
    "        clean_tweet = ' '.join(trimmed_lemma_words)\n",
    "        \n",
    "        return [lemma.lemmatize(x, nltk.corpus.reader.wordnet.VERB) \n",
    "                    for x in nltk.wordpunct_tokenize(clean_tweet) if x not in stop_words]\n",
    "\n",
    "    def cleanHashtags(hashtags):\n",
    "        if hashtags:\n",
    "            hashtags = hashtags.lower()\n",
    "            for pattern in hashtag_patterns:\n",
    "                hashtags = re.sub(pattern, ' ', hashtags)\n",
    "            hashtags = hashtags.strip() \n",
    "        return hashtags\n",
    "\n",
    "\n",
    "    # Clean tweets\n",
    "    tweets_df['clean_tweet_words'] = tweets_df['tweet'].apply(lambda x: cleanTweet(x))\n",
    "    tweets_df['clean_tweet'] = tweets_df['clean_tweet_words'].apply(lambda x:' '.join(x))\n",
    "\n",
    "    # Clean hashtags\n",
    "    tweets_df[\"hashtags\"] = tweets_df[\"hashtags\"].astype(str)\n",
    "    tweets_df[\"hashtags\"] = tweets_df[\"hashtags\"].apply(lambda x: cleanHashtags(x))\n",
    "\n",
    "    # Convert date to datetime and extract month/year\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "    tweets_df['day'] = tweets_df['date'].dt.day\n",
    "    tweets_df['month'] = tweets_df['date'].dt.month\n",
    "\n",
    "    # Remove all tweets which do not have the provided target words\n",
    "    keywords_str = '|'.join(filter_words)\n",
    "    filtered_tweets_df = tweets_df.copy()\n",
    "    filtered_tweets_df = filtered_tweets_df[filtered_tweets_df[\"clean_tweet\"].str.contains(keywords_str)]\n",
    "\n",
    "    return tweets_df, filtered_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_vectors(full_tweets_df):\n",
    "\n",
    "    # Restructure the `clean_text` column into list of list of words\n",
    "    row_sentences = [row for row in full_tweets_df[\"clean_tweet_words\"]]\n",
    "\n",
    "    # Detect common phrases (bigrams) from a list of sentences\n",
    "    phrases = Phrases(row_sentences)\n",
    "    bigram = Phraser(phrases)\n",
    "    sentences = bigram[row_sentences]\n",
    "    \n",
    "    # Initialize vector model\n",
    "    num_cores = multiprocessing.cpu_count()-1\n",
    "    word_vec_model = Word2Vec(workers=num_cores, min_count=1)\n",
    "    \n",
    "    # Establish dataset for the vector model\n",
    "    word_vec_model.build_vocab(sentences)\n",
    "\n",
    "    # Train the model\n",
    "    word_vec_model.train(sentences, epochs=30, total_examples=word_vec_model.corpus_count)\n",
    "\n",
    "    word_vectors = word_vec_model.wv\n",
    "\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_TWEETS, FILTERED_TWEETS = load_data()\n",
    "    \n",
    "WORD_VECS = build_word_vectors(CLEAN_TWEETS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setClusterSentiment(vectors, model, sentiment_mapping, version=0, **kwargs):\n",
    "    display_terms = kwargs.get('display_terms', 20)\n",
    "    print(f'** Top {display_terms} Similar Word Vectors By Cluster **\\n')\n",
    "\n",
    "    collectSamples(key_vectors=vectors, model=model, version=version)\n",
    "\n",
    "    ## Get input\n",
    "    map_string = ', '.join([f'{k} = {v}' for k, v in sentiment_mapping.items()])\n",
    "    print(f\"\\nLabel each cluster: {map_string} (\\\"r\\\" for new samples, \\\"q\\\" to exit)\")\n",
    "    cluster_sentiment_defs = []\n",
    "    user_input = ''\n",
    "    batch_number = 0\n",
    "    valid_sentiment_range = [int(k) for k in sentiment_mapping.keys()]\n",
    "    while len(cluster_sentiment_defs) < len(sentiment_mapping)-1 and user_input != 'q':\n",
    "        \n",
    "        user_input = input(f'Cluster {len(cluster_sentiment_defs)} value:')\n",
    "        if user_input == 'q':\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        if user_input == 'r':\n",
    "            print(f'\\n\\nGenerating next {display_terms} samples...\\n')\n",
    "            batch_number += 1\n",
    "            collectSamples(key_vectors=vectors, model=model, batch=batch_number, version=version)\n",
    "            print('Current state:', cluster_sentiment_defs)\n",
    "            print('Setting cluster:', len(cluster_sentiment_defs))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            value = int(user_input)\n",
    "            if value in cluster_sentiment_defs:\n",
    "                print('Already used this sentiment value.')\n",
    "                continue\n",
    "            elif value not in valid_sentiment_range:\n",
    "                print(f'Value not in provided sentiment mapping: {valid_sentiment_range}')\n",
    "                continue\n",
    "\n",
    "            cluster_sentiment_defs.append(value)\n",
    "            print(f'Set cluster {len(cluster_sentiment_defs)-1} to {value} ({sentiment_mapping[value]})')\n",
    "        except ValueError:\n",
    "            print(f'Must input a number in range {valid_sentiment_range}. Press q to exit')\n",
    "            \n",
    "    cluster_sentiment_defs.append((set(sentiment_mapping.keys()) - set(cluster_sentiment_defs)).pop())\n",
    "    print((f'Set cluster {len(cluster_sentiment_defs)-1} to {cluster_sentiment_defs[-1]} ' +\n",
    "            f'({sentiment_mapping[cluster_sentiment_defs[-1]]})'))\n",
    "    \n",
    "    return cluster_sentiment_defs\n",
    "\n",
    "\n",
    "\n",
    "def collectSamples(key_vectors, model, batch=0, version=0):\n",
    "    num_clusters = model.cluster_centers_.shape[0]\n",
    "    if version == 0:\n",
    "        word_vec_list = [key_vectors.similar_by_vector(model.cluster_centers_[x], \n",
    "                                                    topn=(25 * (batch+1)), \n",
    "                                                    restrict_vocab=None) \n",
    "                                                        for x in range(num_clusters)]\n",
    "    elif version == 1:\n",
    "        pca_values = key_vectors['pca_values']\n",
    "        pca_values = np.stack(pca_values.to_numpy().ravel()).reshape(len(key_vectors), 3)\n",
    "\n",
    "        word_vec_list = []\n",
    "        for x in range(num_clusters):\n",
    "            similarities = cosine_similarity(pca_values, model.cluster_centers_[x].reshape(1, -1)).flatten()\n",
    "\n",
    "            top_indicies = np.argpartition(similarities, kth=-25, axis=0)[-25:]\n",
    "\n",
    "            word_vec_list.append(list(zip(list(key_vectors.iloc[top_indicies]['words']), similarities[top_indicies])))\n",
    "\n",
    "\n",
    "    cluster_values = np.array(list(zip(*[x[(25 * batch):] for x in word_vec_list])))\n",
    "    cluster_cols = [f'Cluster {x}' for x in range(num_clusters)]\n",
    "\n",
    "    # # Collect terms spanning multiple clusters for deciphering\n",
    "    term_freq, counts = np.unique([x[0] for x in np.vstack(cluster_values)], axis=0, return_counts=True)\n",
    "    unique_terms = term_freq[counts == 1]\n",
    "\n",
    "    # Separate unique from duplicate terms\n",
    "    unique_cluster_vals = [[] for _ in range(num_clusters)]\n",
    "    shared_cluster_vals = defaultdict(lambda : [0] * num_clusters)\n",
    "\n",
    "    for ix, iy in np.ndindex(cluster_values.shape[:2]):\n",
    "        term, vec = cluster_values[ix, iy]\n",
    "        if term in unique_terms:\n",
    "            unique_cluster_vals[iy].append((term, float(vec)))\n",
    "        else:\n",
    "            shared_cluster_vals[term][iy] = float(vec)\n",
    "\n",
    "\n",
    "    print('Unique Terms from Clusters')\n",
    "    max_num_unique = max(len(c) for c in unique_cluster_vals)\n",
    "\n",
    "    # Sort by and drop vector. Even out column lengths\n",
    "    unique_cluster_terms = np.array([[val[0] for val in sorted(cluster, key=lambda x: x[1])] + \n",
    "                                        ['']*(max_num_unique-len(cluster)) # Adjust lengths\n",
    "                                            for cluster in unique_cluster_vals])\n",
    "\n",
    "    unique_terms_df = pd.DataFrame(unique_cluster_terms.T, columns=cluster_cols)\n",
    "    display(unique_terms_df)\n",
    "\n",
    "    print('\\nDuplicate Terms from Clusters')\n",
    "    if shared_cluster_vals:\n",
    "        # Build dict for scaling\n",
    "        shared_vals_df = pd.DataFrame.from_dict(shared_cluster_vals, orient='index', \n",
    "                                                    columns=cluster_cols).reset_index()\n",
    "\n",
    "        display(shared_vals_df)\n",
    "        # # Calc differences between clusters (for interpretation purposes)\n",
    "        # for c in cluster_cols[1:]:\n",
    "        #     shared_vals_df[f'{c} relative to {cluster_cols[0]}'] = shared_vals_df[cluster_cols[0]] - shared_vals_df[c]\n",
    "\n",
    "        # shared_vals_df = shared_vals_df.drop(cluster_cols[1:], axis=1)\n",
    "\n",
    "        # scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        # scaled_cols = [c for c in shared_vals_df.columns if c not in ['index', cluster_cols[0]]]\n",
    "\n",
    "        # scaled_data = scaler.fit_transform(shared_vals_df[scaled_cols])\n",
    "        # scaled_df = pd.DataFrame(scaled_data, columns=scaled_cols)\n",
    "        # duplicate_terms_df = pd.merge(shared_vals_df[['index', cluster_cols[0]]], \n",
    "        #                                 scaled_df, left_index=True, right_index=True) \\\n",
    "        #                                     .sort_values(by=cluster_cols[0])\n",
    "\n",
    "        # col_map = { cluster_cols[0]: f'{cluster_cols[0]} (baseline)', 'index': 'term'}\n",
    "        # duplicate_terms_df = duplicate_terms_df.rename(columns=col_map).set_index('term')\n",
    "        # display(duplicate_terms_df)\n",
    "    else:\n",
    "        print('\\tNo duplicates between clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OG_cluster(word_vectors, tweets_df=None):\n",
    "    cluster_model = KMeans(**{\n",
    "            \"n_clusters\": 3,\n",
    "            \"max_iter\": 1000,\n",
    "            \"n_init\": 50,\n",
    "            \"random_state\": 42\n",
    "        })\n",
    "    \n",
    "    cluster_model = cluster_model.fit(X=word_vectors.vectors.astype('double'))\n",
    "\n",
    "    # cluster_sentiment_defs = setClusterSentiment(word_vectors, cluster_model, sentiment_map, version=0)\n",
    "\n",
    "    # Create a DataFrame of words with their embeddings and cluster values\n",
    "    words_df = pd.DataFrame(word_vectors.index_to_key, columns=['words'])\n",
    "    words_df['vectors'] = words_df.words.apply(lambda x: word_vectors[str(x)])\n",
    "    words_df['predicted_cluster'] = words_df.vectors.apply(lambda x: cluster_model.predict([np.array(x)]))\n",
    "    words_df.predicted_cluster = words_df.predicted_cluster.apply(lambda x: x[0])\n",
    "\n",
    "    # words_df['cluster_value'] = [cluster_sentiment_defs[i] for i in words_df.cluster]\n",
    "\n",
    "    # # Calculate proximity of words in each vector\n",
    "    # calc_vector_nearness = lambda x: 1 / (cluster_model.transform([x.vectors]).min())\n",
    "    # words_df['closeness_score'] = words_df.apply(calc_vector_nearness, axis=1)\n",
    "    # words_df['sentiment_coeff'] = words_df.closeness_score * words_df.cluster_value\n",
    "\n",
    "    # # Map sentiment encodings\n",
    "    # words_df[\"sentiment\"] = words_df[\"cluster_value\"].map(sentiment_map)\n",
    "\n",
    "    # words_cluster_dict = dict(zip(words_df.words, words_df.cluster_value))    \n",
    "\n",
    "    # def getSentiment(row):\n",
    "    #     words_list = row['clean_tweet_words']\n",
    "    #     total = sum(int(words_cluster_dict.get(word, 0)) for word in words_list)\n",
    "    #     avg = total / len(words_list)\n",
    "    #     return -1 if (avg < -0.15) else 1 if (avg > 0.15) else 0\n",
    "\n",
    "    # # Add sentiment column (integer values)\n",
    "    # tweets_df[\"sentiment_val\"] = tweets_df.apply(getSentiment, axis=1)\n",
    "    # # Map integer sentiment to word value\n",
    "    # tweets_df[\"sentiment\"] = tweets_df[\"sentiment_val\"].map(sentiment_map)\n",
    "\n",
    "\n",
    "    return words_df, cluster_model\n",
    "\n",
    "og_cluster_df, og_model = OG_cluster(WORD_VECS)\n",
    "og_cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_cluster_df['predicted_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option \\#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(WORD_VECS.vectors).astype('double')\n",
    "\n",
    "# Used preprocessing module of sklearn library to scale data.\n",
    "X_scaled = preprocessing.scale(df)\n",
    "\n",
    "# I have used StandarScaler() & fit_transform() function of sklearn library to standardize features.\n",
    "X_std = StandardScaler().fit_transform(X_scaled)\n",
    "\n",
    "# std_df = pd.DataFrame(X_std)\n",
    "\n",
    "# # base_pca = PCA(n_components = 3) # Using PCA to remove cols which has less co-relation\n",
    "# word_vectors_pca = PCA(n_components = 3).fit(std_df) #fit_transform() is used to scale training data to learn parameters such as \n",
    "# pca_values = word_vectors_pca.transform(std_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pca = PCA(n_components = 3)\n",
    "word_vectors_pca = PCA(n_components = 3).fit(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_vals = word_vectors_pca.transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = X_std.reshape(len(X_std), 1, -1)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(std_df.to_numpy().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(word_vectors, query_vector):\n",
    "    return cdist(word_vectors, query_vector, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ACCEPTED_DISTANCE = 0.1\n",
    "\n",
    "def Opt1_cluster(word_vectors, tweets_df=None):\n",
    "    df = pd.DataFrame(word_vectors.vectors).astype('double')\n",
    "\n",
    "    # Used preprocessing module of sklearn library to scale data.\n",
    "    X_scaled = preprocessing.scale(df)\n",
    "\n",
    "    # I have used StandarScaler() & fit_transform() function of sklearn library to standardize features.\n",
    "    X_std = StandardScaler().fit_transform(X_scaled)\n",
    "\n",
    "    # base_pca = PCA(n_components = 3) # Using PCA to remove cols which has less co-relation\n",
    "    word_vectors_pca = PCA(n_components = 3).fit(X_std) #fit_transform() is used to scale training data to learn parameters such as \n",
    "    pca_values = word_vectors_pca.transform(X_std)\n",
    "    \n",
    "    words_df = pd.DataFrame({ \n",
    "                            'words': word_vectors.index_to_key, \n",
    "                            'vectors': X_std,\n",
    "                            'pca_values': pca_values\n",
    "                        })\n",
    "\n",
    "    # mean & variance of the features of training set and then these parameters are used to scale our testing data.\n",
    "    # As concluded using Elbow Method.\n",
    "    n_clusters = 3\n",
    "    kmeans = KMeans(n_clusters= n_clusters, max_iter=400)# Partition 'n' no. of observations into 'k' no. of clusters. \n",
    "    fit_kmeans = kmeans.fit(pca_values) # Fitting k-means model  to feature array\n",
    "\n",
    "    # cluster_sentiment_defs = setClusterSentiment(words_df, fit_kmeans, sentiment_map, version=1)\n",
    "\n",
    "    words_df['predicted_cluster'] = fit_kmeans.predict(np.stack(words_df.pca_values.to_numpy() \\\n",
    "                                                .ravel()).reshape(len(words_df), 3))\n",
    "\n",
    "    to_cluster_space = lambda x: fit_kmeans.transform([x]).flatten()\n",
    "    words_df['cluster_space'] = words_df.pca_values.apply(to_cluster_space)\n",
    "\n",
    "    # grouped_cluster_space = np.flip(words_df.groupby(['predicted_cluster'], sort=False) \\\n",
    "    #                                 .cluster_space.apply(lambda x: np.array(x)).to_numpy())\n",
    "    # cluster_space_matricies = [np.array(list(map(lambda r: r[0], x))).reshape(-1, 1) for x in grouped_cluster_space]\n",
    "\n",
    "    def score_clusters(row):\n",
    "        \n",
    "        query_vec = row.cluster_space.reshape(-1, 1, 1)\n",
    "\n",
    "        zipped_cluster_query = zip(cluster_space_matricies, query_vec)\n",
    "\n",
    "        cluster_distances = [euclidean_dist(cluster_space, query) for (cluster_space, query) \n",
    "                                in zipped_cluster_query] # distance from this sample to all others\n",
    "        \n",
    "        calc_dist_to_clusters = [np.mean(d[d != 0]) for d in cluster_distances] # between this sample and all other clusters\n",
    "        predict_dist_to_clusters = euclidean_dist(fit_kmeans.cluster_centers_, row.cluster_space.reshape(1, -1)).flatten()\n",
    "\n",
    "\n",
    "        calculated_cluster = np.argmin(calc_dist_to_clusters)\n",
    "        calculated_shortest_dist = calc_dist_to_clusters[calculated_cluster]\n",
    "        predicted_shortest_dist = row.cluster_space[row.predicted_cluster]\n",
    "\n",
    "        \n",
    "        distance_diffs = (predict_dist_to_clusters - calc_dist_to_clusters)\n",
    "\n",
    "\n",
    "        if row.predicted_cluster != calculated_cluster:\n",
    "            error = (((predicted_shortest_dist - calc_dist_to_clusters[row.predicted_cluster]) + # Difference from predicted cluster\n",
    "                        (row.cluster_space[calculated_cluster] - calculated_shortest_dist)) / 2) # Difference from calculated cluster\n",
    "            distance_diffs *= (error)\n",
    "            \n",
    "        \n",
    "        cluster_similarity_scores =  1 / np.array(calc_dist_to_clusters)\n",
    "        confidence_scores = 1 / distance_diffs\n",
    "\n",
    "        row['cluster_similarity'] = cluster_similarity_scores # similarity between this vector and each cluster\n",
    "        row['confidence_scores'] = confidence_scores \n",
    "        return row\n",
    "    \n",
    "\n",
    "    # scored_clusters = words_df.apply(score_clusters, axis=1)\n",
    "\n",
    "    # scoring_dimensions = ['cluster_space', 'cluster_scores', 'confidence_score']\n",
    "    # words_clusters_df = scored_clusters[['words', 'predicted_cluster']+scoring_dimensions]\n",
    "    # words_clusters_df = words_clusters_df.set_index('words')\n",
    "\n",
    "\n",
    "    return words_df, fit_kmeans\n",
    "\n",
    "\n",
    "opt1_cluster_df, opt1_model = Opt1_cluster(WORD_VECS)\n",
    "opt1_cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.flip(opt1_cluster_df.groupby(['predicted_cluster'], sort=False) \\\n",
    "                                .pca_values.apply(lambda x: np.array(x)).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_space_matricies = [np.array(list(map(lambda r: r[i], x))).reshape(-1, 1) for i, x in enumerate(grouped_cluster_space)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = opt1_cluster_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_cluster_space = np.flip(opt1_cluster_df.groupby(['predicted_cluster'], sort=False) \\\n",
    "                                .cluster_space.apply(lambda x: np.array(x)).to_numpy())\n",
    "cluster_space_matricies = [np.array(list(map(lambda r: r[i], x))).reshape(-1, 1) for i, x in enumerate(grouped_cluster_space)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_cluster_space[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(map(lambda r: r[0], grouped_cluster_space[0])))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(map(lambda r: r[0], grouped_cluster_space[1])))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = row.cluster_space.reshape(-1, 1, 1)\n",
    "\n",
    "zipped_cluster_query = zip(cluster_space_matricies, query_vec)\n",
    "\n",
    "cluster_distances = [euclidean_dist(cluster_space, query) for (cluster_space, query) \n",
    "                        in zipped_cluster_query] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.cluster_space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_dist_to_clusters = [np.mean(d[d != 0]) for d in cluster_distances] # between this sample and all other clusters\n",
    "predict_dist_to_clusters = euclidean_dist(opt1_model.cluster_centers_, row.cluster_space.reshape(1, -1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dist_to_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_dist_to_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 2]\n",
    "b = np.array([2, 3, 4])\n",
    "\n",
    "z = abs(a - b)\n",
    "z += 2\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1_cluster_df['predicted_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "mm_scaled = mm_scaler.fit_transform(opt1_cluster_df.confidence_score.to_numpy().reshape(-1, 1))\n",
    "mm_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "std_scaled = std_scaler.fit_transform(opt1_cluster_df.confidence_score.to_numpy().reshape(-1, 1))\n",
    "std_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_scaler = RobustScaler()\n",
    "rob_scaled = rob_scaler.fit_transform(opt1_cluster_df.confidence_score.to_numpy().reshape(-1, 1))\n",
    "rob_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_scaler = MaxAbsScaler()\n",
    "abs_scaled = abs_scaler.fit_transform(opt1_cluster_df.confidence_score.to_numpy().reshape(-1, 1))\n",
    "abs_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('robust', RobustScaler()), ('minmax', MinMaxScaler(feature_range=(0,1)))])\n",
    "pipe_scaled = pipe.fit_transform(opt1_cluster_df.confidence_score.to_numpy().reshape(-1, 1))\n",
    "pipe_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(opt1_cluster_df.confidence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(std_scaled.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(mm_scaled.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(rob_scaled.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(abs_scaled.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(pipe_scaled.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(opt1_cluster_df.confidence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_lookup_df = words_clusters_df[['cluster_scores', 'cluster_space']]\n",
    "# grouping_features = ['cluster_scores', 'cluster_space']\n",
    "scoring_dimensions = ['cluster_space', 'cluster_scores', 'confidence_score']\n",
    "words_clusters_df = opt1_cluster_df[['words', 'predicted_cluster']+scoring_dimensions]\n",
    "words_clusters_df = words_clusters_df.set_index('words')\n",
    "\n",
    "def clusterGrouping(row):\n",
    "    words = row.clean_tweet_words\n",
    "    # words_search = np.stack(words_clusters_df.loc[words][scoring_dimensions].to_numpy().ravel()).reshape(3, -1, 3)\n",
    "    words_search = words_clusters_df.loc[words][scoring_dimensions]\n",
    "    distances, similarities = np.stack(words_search[['cluster_space', 'cluster_scores']].to_numpy().ravel()).reshape(2, -1, 3)\n",
    "    confidences = words_search['confidence_score'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    weighted_distances = similarities * distances * confidences\n",
    "\n",
    "    closest_clusters = np.argmin(weighted_distances, axis=1)\n",
    "\n",
    "    agg_weights = [[] for _ in range(3)]\n",
    "    for cluster, weights in zip(closest_clusters, weighted_distances):\n",
    "        agg_weights[cluster].append(weights[cluster])\n",
    "\n",
    "    avg_weights = [np.mean(closest_weights) if closest_weights else np.nan for closest_weights in agg_weights]\n",
    "    closest_cluster = np.argmin(avg_weights)\n",
    "\n",
    "    return closest_cluster, avg_weights[closest_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = FILTERED_TWEETS.iloc[1].clean_tweet_words\n",
    "# words_search = np.stack(words_clusters_df.loc[words][scoring_dimensions].to_numpy().ravel()).reshape(3, -1, 3)\n",
    "words_search = words_clusters_df.loc[words][scoring_dimensions]\n",
    "distances, similarities = np.stack(words_search[['cluster_space', 'cluster_scores']].to_numpy().ravel()).reshape(2, -1, 3)\n",
    "confidences = words_search['confidence_score'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "weighted_distances = similarities * distances * confidences\n",
    "\n",
    "closest_clusters = np.argmax(weighted_distances, axis=1)\n",
    "\n",
    "agg_weights = [[] for _ in range(3)]\n",
    "for cluster, weights in zip(closest_clusters, weighted_distances):\n",
    "    agg_weights[cluster].append(weights[cluster])\n",
    "\n",
    "avg_weights = [np.mean(closest_weights) for closest_weights in agg_weights if closest_weights]\n",
    "closest_cluster = np.argmax(avg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterGrouping(FILTERED_TWEETS.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERED_TWEETS[:10].apply(clusterGrouping, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kmeans_clustering(Y_sklearn, fitted):\n",
    "#     \"\"\"\n",
    "#     This function will predict clusters on training set and plot the visuals of clusters as well.\n",
    "#     \"\"\"\n",
    "\n",
    "#     plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction ,s=50, cmap='viridis') # Plotting scatter plot \n",
    "#     centers2 = fitted.cluster_centers_ # It will give best possible coordinates of cluster center after fitting k-means\n",
    "#     plt.scatter(centers2[:, 0], centers2[:, 1],c='black', s=300, alpha=0.6);\n",
    "#     # As this can be seen from the figure, there is an outlier as well.\n",
    "    \n",
    "# kmeans_clustering(Y_sklearn, fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_cluster(ngrams, X_std, prediction, n_feats):\n",
    "    # Get unique labels, in this case {0,1}\n",
    "    labels = np.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = np.where(prediction==label) # Get indices for each feature corresponding to each cluster.        \n",
    "        x_means = np.mean(X_std[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        # features = n_grams_to_use\n",
    "        best_features = [(ngrams[i], x_means[i]) for i in sorted_means] # Retrieve corresponding best features to that of best scores.\n",
    "        Df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(Df) # append both the Dataframes to a list\n",
    "    return dfs\n",
    "\n",
    "dfs = get_top_features_cluster(wv.index_to_key, X_std, prediction, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[:25][0]) # Get top 25 rows of 1st Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[:25][1]) # Get top 25 rows of 2nd Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[:25][2]) # Get top 25 rows of 3nd Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(dfs):\n",
    "    \"\"\"\n",
    "    This function will print combined bar graphs for all the possible clusters.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(14,12))\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.set_title(\"Cluster: \"+ str(i), fontsize = 14)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.score, align='center', color='#40826d')\n",
    "        yticks = ax.set_yticklabels(df.features)\n",
    "    plt.show();\n",
    "\n",
    "plot_features(dfs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d883baf2d2c020187d16fb74e1bc85e676b385dd78044a08a209b4abcafece"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
