{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale as sk_scale\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prep nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup bare-minimum config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_patterns = [\n",
    "            r\"https?:\\\\/\\\\/[a-zA-Z0-9@:%._\\\\/+~#=?&;-]*\",\n",
    "            r\"\\\\$[a-zA-Z0-9]*\",\n",
    "            r\"\\\\@[a-zA-Z0-9]*\",\n",
    "            r\"[^a-zA-Z\\\\\\\"]\"\n",
    "        ]\n",
    "\n",
    "hashtag_patterns = [\n",
    "            r\"\\\\$[a-zA-Z0-9]*\",\n",
    "            r\"[^a-zA-Z\\\\\\\"]\"\n",
    "        ]\n",
    "\n",
    "column_mappings = {\n",
    "            \"date\": \"date\",\n",
    "            \"user_name\": \"username\",\n",
    "            \"retweets\": \"retweets\",\n",
    "            \"text\": \"tweet\",\n",
    "            \"hashtags\": \"hashtags\"\n",
    "        }\n",
    "\n",
    "filter_words = [\n",
    "            \"ukraine\",\n",
    "            \"russia\",\n",
    "            \"zelensky\"\n",
    "        ]\n",
    "\n",
    "sentiment_map = {\n",
    "            -1: \"negative\",\n",
    "            0: \"neutral\",\n",
    "            1: \"positive\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in some data and build word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    import_version = 12\n",
    "    import_dest = \"../../data/slava_ukraini_tweets_v{version}.csv\".format(version=import_version)\n",
    "    raw_tweets_df = pd.read_csv(import_dest)\n",
    "\n",
    "\n",
    "    # Rename columns\n",
    "    tweets_df = raw_tweets_df[list(column_mappings.keys())].rename(columns=column_mappings)\n",
    "\n",
    "    # Drop duplicate tweets\n",
    "    tweets_df = tweets_df.drop_duplicates(subset='tweet', keep='first')\n",
    "\n",
    "    # Initialize Lemmatizer\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    def cleanTweet(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        for pattern in tweets_patterns:\n",
    "            tweet = re.sub(pattern, ' ', tweet)\n",
    "        tweet = ' '.join([w for w in tweet.split() if len(w) > 1])\n",
    "        \n",
    "        trimmed_lemma_words = [lemma.lemmatize(x) for x in nltk.wordpunct_tokenize(tweet) \n",
    "                                    if x not in stop_words]\n",
    "        clean_tweet = ' '.join(trimmed_lemma_words)\n",
    "        \n",
    "        return [lemma.lemmatize(x, nltk.corpus.reader.wordnet.VERB) \n",
    "                    for x in nltk.wordpunct_tokenize(clean_tweet) if x not in stop_words]\n",
    "\n",
    "    def cleanHashtags(hashtags):\n",
    "        if hashtags:\n",
    "            hashtags = hashtags.lower()\n",
    "            for pattern in hashtag_patterns:\n",
    "                hashtags = re.sub(pattern, ' ', hashtags)\n",
    "            hashtags = hashtags.strip() \n",
    "        return hashtags\n",
    "\n",
    "\n",
    "    # Clean tweets\n",
    "    tweets_df['clean_tweet_words'] = tweets_df['tweet'].apply(lambda x: cleanTweet(x))\n",
    "    tweets_df['clean_tweet'] = tweets_df['clean_tweet_words'].apply(lambda x:' '.join(x))\n",
    "\n",
    "    # Clean hashtags\n",
    "    tweets_df[\"hashtags\"] = tweets_df[\"hashtags\"].astype(str)\n",
    "    tweets_df[\"hashtags\"] = tweets_df[\"hashtags\"].apply(lambda x: cleanHashtags(x))\n",
    "\n",
    "    # Convert date to datetime and extract month/year\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "    tweets_df['day'] = tweets_df['date'].dt.day\n",
    "    tweets_df['month'] = tweets_df['date'].dt.month\n",
    "\n",
    "    # Remove all tweets which do not have the provided target words\n",
    "    keywords_str = '|'.join(filter_words)\n",
    "    filtered_tweets_df = tweets_df.copy()\n",
    "    filtered_tweets_df = filtered_tweets_df[filtered_tweets_df[\"clean_tweet\"].str.contains(keywords_str)]\n",
    "\n",
    "    return tweets_df, filtered_tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_vectors(full_tweets_df):\n",
    "\n",
    "    # Restructure the `clean_text` column into list of list of words\n",
    "    row_sentences = [row for row in full_tweets_df[\"clean_tweet_words\"]]\n",
    "\n",
    "    # Detect common phrases (bigrams) from a list of sentences\n",
    "    phrases = Phrases(row_sentences)\n",
    "    bigram = Phraser(phrases)\n",
    "    sentences = bigram[row_sentences]\n",
    "    \n",
    "    # Initialize vector model\n",
    "    num_cores = multiprocessing.cpu_count()-1\n",
    "    word_vec_model = Word2Vec(workers=num_cores, min_count=1)\n",
    "    \n",
    "    # Establish dataset for the vector model\n",
    "    word_vec_model.build_vocab(sentences)\n",
    "\n",
    "    # Train the model\n",
    "    word_vec_model.train(sentences, epochs=30, total_examples=word_vec_model.corpus_count)\n",
    "\n",
    "    word_vectors = word_vec_model.wv\n",
    "\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_TWEETS, FILTERED_TWEETS = load_data()\n",
    "    \n",
    "WORD_VECS = build_word_vectors(CLEAN_TWEETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate same sentiment assignment system (for easy integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setClusterSentiment(vectors, model, sentiment_mapping, version=0, **kwargs):\n",
    "    display_terms = kwargs.get('display_terms', 20)\n",
    "    print(f'** Top {display_terms} Similar Word Vectors By Cluster **\\n')\n",
    "\n",
    "    collectSamples(key_vectors=vectors, model=model, version=version)\n",
    "\n",
    "    ## Get input\n",
    "    map_string = ', '.join([f'{k} = {v}' for k, v in sentiment_mapping.items()])\n",
    "    print(f\"\\nLabel each cluster: {map_string} (\\\"r\\\" for new samples, \\\"q\\\" to exit)\")\n",
    "    cluster_sentiment_defs = []\n",
    "    user_input = ''\n",
    "    batch_number = 0\n",
    "    valid_sentiment_range = [int(k) for k in sentiment_mapping.keys()]\n",
    "    while len(cluster_sentiment_defs) < len(sentiment_mapping)-1 and user_input != 'q':\n",
    "        \n",
    "        user_input = input(f'Cluster {len(cluster_sentiment_defs)} value:')\n",
    "        if user_input == 'q':\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        if user_input == 'r':\n",
    "            print(f'\\n\\nGenerating next {display_terms} samples...\\n')\n",
    "            batch_number += 1\n",
    "            collectSamples(key_vectors=vectors, model=model, batch=batch_number, version=version)\n",
    "            print('Current state:', cluster_sentiment_defs)\n",
    "            print('Setting cluster:', len(cluster_sentiment_defs))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            value = int(user_input)\n",
    "            if value in cluster_sentiment_defs:\n",
    "                print('Already used this sentiment value.')\n",
    "                continue\n",
    "            elif value not in valid_sentiment_range:\n",
    "                print(f'Value not in provided sentiment mapping: {valid_sentiment_range}')\n",
    "                continue\n",
    "\n",
    "            cluster_sentiment_defs.append(value)\n",
    "            print(f'Set cluster {len(cluster_sentiment_defs)-1} to {value} ({sentiment_mapping[value]})')\n",
    "        except ValueError:\n",
    "            print(f'Must input a number in range {valid_sentiment_range}. Press q to exit')\n",
    "            \n",
    "    cluster_sentiment_defs.append((set(sentiment_mapping.keys()) - set(cluster_sentiment_defs)).pop())\n",
    "    print((f'Set cluster {len(cluster_sentiment_defs)-1} to {cluster_sentiment_defs[-1]} ' +\n",
    "            f'({sentiment_mapping[cluster_sentiment_defs[-1]]})'))\n",
    "    \n",
    "    return cluster_sentiment_defs\n",
    "\n",
    "\n",
    "\n",
    "def collectSamples(key_vectors, model, batch=0, version=0):\n",
    "    num_clusters = model.cluster_centers_.shape[0]\n",
    "    if version == 0:\n",
    "        word_vec_list = [key_vectors.similar_by_vector(model.cluster_centers_[x], \n",
    "                                                    topn=(25 * (batch+1)), \n",
    "                                                    restrict_vocab=None) \n",
    "                                                        for x in range(num_clusters)]\n",
    "    elif version == 1:\n",
    "        pca_values = key_vectors['pca_values']\n",
    "        pca_values = np.stack(pca_values.to_numpy().ravel()).reshape(len(key_vectors), 3)\n",
    "\n",
    "        word_vec_list = []\n",
    "        for x in range(num_clusters):\n",
    "            similarities = cosine_similarity(pca_values, model.cluster_centers_[x].reshape(1, -1)).flatten()\n",
    "\n",
    "            top_indicies = np.argpartition(similarities, kth=-25, axis=0)[-25:]\n",
    "\n",
    "            word_vec_list.append(list(zip(list(key_vectors.iloc[top_indicies]['words']), similarities[top_indicies])))\n",
    "\n",
    "\n",
    "    cluster_values = np.array(list(zip(*[x[(25 * batch):] for x in word_vec_list])))\n",
    "    cluster_cols = [f'Cluster {x}' for x in range(num_clusters)]\n",
    "\n",
    "    # # Collect terms spanning multiple clusters for deciphering\n",
    "    term_freq, counts = np.unique([x[0] for x in np.vstack(cluster_values)], axis=0, return_counts=True)\n",
    "    unique_terms = term_freq[counts == 1]\n",
    "\n",
    "    # Separate unique from duplicate terms\n",
    "    unique_cluster_vals = [[] for _ in range(num_clusters)]\n",
    "    shared_cluster_vals = defaultdict(lambda : [0] * num_clusters)\n",
    "\n",
    "    for ix, iy in np.ndindex(cluster_values.shape[:2]):\n",
    "        term, vec = cluster_values[ix, iy]\n",
    "        if term in unique_terms:\n",
    "            unique_cluster_vals[iy].append((term, float(vec)))\n",
    "        else:\n",
    "            shared_cluster_vals[term][iy] = float(vec)\n",
    "\n",
    "\n",
    "    print('Unique Terms from Clusters')\n",
    "    max_num_unique = max(len(c) for c in unique_cluster_vals)\n",
    "\n",
    "    # Sort by and drop vector. Even out column lengths\n",
    "    unique_cluster_terms = np.array([[val[0] for val in sorted(cluster, key=lambda x: x[1])] + \n",
    "                                        ['']*(max_num_unique-len(cluster)) # Adjust lengths\n",
    "                                            for cluster in unique_cluster_vals])\n",
    "\n",
    "    unique_terms_df = pd.DataFrame(unique_cluster_terms.T, columns=cluster_cols)\n",
    "    display(unique_terms_df)\n",
    "\n",
    "    print('\\nDuplicate Terms from Clusters')\n",
    "    if shared_cluster_vals:\n",
    "        # Build dict for scaling\n",
    "        shared_vals_df = pd.DataFrame.from_dict(shared_cluster_vals, orient='index', \n",
    "                                                    columns=cluster_cols).reset_index()\n",
    "\n",
    "        display(shared_vals_df)\n",
    "        # # Calc differences between clusters (for interpretation purposes)\n",
    "        # for c in cluster_cols[1:]:\n",
    "        #     shared_vals_df[f'{c} relative to {cluster_cols[0]}'] = shared_vals_df[cluster_cols[0]] - shared_vals_df[c]\n",
    "\n",
    "        # shared_vals_df = shared_vals_df.drop(cluster_cols[1:], axis=1)\n",
    "\n",
    "        # scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        # scaled_cols = [c for c in shared_vals_df.columns if c not in ['index', cluster_cols[0]]]\n",
    "\n",
    "        # scaled_data = scaler.fit_transform(shared_vals_df[scaled_cols])\n",
    "        # scaled_df = pd.DataFrame(scaled_data, columns=scaled_cols)\n",
    "        # duplicate_terms_df = pd.merge(shared_vals_df[['index', cluster_cols[0]]], \n",
    "        #                                 scaled_df, left_index=True, right_index=True) \\\n",
    "        #                                     .sort_values(by=cluster_cols[0])\n",
    "\n",
    "        # col_map = { cluster_cols[0]: f'{cluster_cols[0]} (baseline)', 'index': 'term'}\n",
    "        # duplicate_terms_df = duplicate_terms_df.rename(columns=col_map).set_index('term')\n",
    "        # display(duplicate_terms_df)\n",
    "    else:\n",
    "        print('\\tNo duplicates between clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V1 - Original method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v1_SentimentGenerator(word_vectors, tweets_df=None):\n",
    "    cluster_model = KMeans(**{\n",
    "            \"n_clusters\": 3,\n",
    "            \"max_iter\": 1000,\n",
    "            \"n_init\": 50,\n",
    "            \"random_state\": 42\n",
    "        })\n",
    "    \n",
    "    cluster_model = cluster_model.fit(X=word_vectors.vectors.astype('double'))\n",
    "\n",
    "    # cluster_sentiment_defs = setClusterSentiment(word_vectors, cluster_model, sentiment_map, version=0)\n",
    "\n",
    "    # Create a DataFrame of words with their embeddings and cluster values\n",
    "    words_df = pd.DataFrame(word_vectors.index_to_key, columns=['words'])\n",
    "    words_df['vectors'] = words_df.words.apply(lambda x: word_vectors[str(x)])\n",
    "    words_df['predicted_cluster'] = words_df.vectors.apply(lambda x: cluster_model.predict([np.array(x)]))\n",
    "    words_df.predicted_cluster = words_df.predicted_cluster.apply(lambda x: x[0])\n",
    "\n",
    "    # words_df['cluster_value'] = [cluster_sentiment_defs[i] for i in words_df.cluster]\n",
    "\n",
    "    # # Calculate proximity of words in each vector\n",
    "    # calc_vector_nearness = lambda x: 1 / (cluster_model.transform([x.vectors]).min())\n",
    "    # words_df['closeness_score'] = words_df.apply(calc_vector_nearness, axis=1)\n",
    "    # words_df['sentiment_coeff'] = words_df.closeness_score * words_df.cluster_value\n",
    "\n",
    "    # # Map sentiment encodings\n",
    "    # words_df[\"sentiment\"] = words_df[\"cluster_value\"].map(sentiment_map)\n",
    "\n",
    "    # words_cluster_dict = dict(zip(words_df.words, words_df.cluster_value))    \n",
    "\n",
    "    # def getSentiment(row):\n",
    "    #     words_list = row['clean_tweet_words']\n",
    "    #     total = sum(int(words_cluster_dict.get(word, 0)) for word in words_list)\n",
    "    #     avg = total / len(words_list)\n",
    "    #     return -1 if (avg < -0.15) else 1 if (avg > 0.15) else 0\n",
    "\n",
    "    # # Add sentiment column (integer values)\n",
    "    # tweets_df[\"sentiment_val\"] = tweets_df.apply(getSentiment, axis=1)\n",
    "    # # Map integer sentiment to word value\n",
    "    # tweets_df[\"sentiment\"] = tweets_df[\"sentiment_val\"].map(sentiment_map)\n",
    "\n",
    "\n",
    "    return words_df, cluster_model\n",
    "\n",
    "og_cluster_df, og_model = v1_SentimentGenerator(WORD_VECS)\n",
    "og_cluster_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 - No more row-by-row `apply()`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################## Helper Functions ##########################\n",
    "\n",
    "# Used for transforming the predicted_cluster groups from the words dataframe\n",
    "def explodeAndPad(series_group, cluster_dims):\n",
    "    exploded_arr = pd.DataFrame(series_group.tolist(), series_group.index).to_numpy()\n",
    "    z = np.zeros((max(cluster_dims), len(cluster_dims)))\n",
    "    z[0:len(exploded_arr)] = exploded_arr\n",
    "    return z\n",
    "\n",
    "\n",
    "# Manipulates the full dataset to possess an axis for cluster value\n",
    "def generateClusterQueries(cluster_matrix):\n",
    "    cluster_queries = []\n",
    "    for i in range(len(cluster_matrix)):\n",
    "        raw_values = cluster_matrix[i]\n",
    "        query = raw_values[np.newaxis, :, :]\n",
    "        cluster_queries.append(query)\n",
    "    return np.squeeze(np.array(cluster_queries), axis=1)\n",
    "\n",
    "\n",
    "# Utilizes the euclidean distance algorithm to find the distance between two matricies\n",
    "def calcDistance(mat_A, mat_B):\n",
    "\n",
    "    def applyEuclidean3d(subset, axis):\n",
    "        distance = euclidean3d(mat_A, subset).T[np.newaxis, :, :]\n",
    "        return np.repeat(distance, 3, 0)\n",
    "    \n",
    "    if mat_A.ndim == 3 and mat_B.ndim == 3:\n",
    "        return np.apply_over_axes(applyEuclidean3d, mat_B, (0,1))\n",
    "\n",
    "    elif mat_A.ndim == 2 and mat_B.ndim == 2:\n",
    "        return np.apply_over_axes(\n",
    "                    ( lambda subset, _: euclidean3d(mat_A[:,None,:], subset).T ), \n",
    "                        mat_B, (0, 1))\n",
    "  \n",
    "        \n",
    "# numpy.einsum implementation of to find the euclidean distance between \n",
    "# two [i x j x k] matricies\n",
    "def euclidean3d(mat_A, mat_B):\n",
    "    subs = mat_A - mat_B\n",
    "    return np.sqrt(np.einsum('ijk,ijk->ij', subs, subs))\n",
    "\n",
    "\n",
    "# Applies a scaler to a matrix \n",
    "def scale3d(mat_A, scaler=None, restore=False):\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler()\n",
    "    original_shape = list(mat_A.shape)\n",
    "    flattened_mat = mat_A.reshape(-1, original_shape[len(original_shape)-1])\n",
    "    scaled_mat = scaler.fit_transform(flattened_mat)\n",
    "    return scaled_mat if not restore else scaled_mat.reshape(*original_shape)\n",
    "\n",
    "\n",
    "# Various numpy.einsum implemenations of matrix multiplication\n",
    "matrixDot2d = lambda mat_A, mat_B: np.einsum('ij,jk->ik', mat_A, mat_B)\n",
    "matrixDot3d = lambda mat_A, mat_B: np.einsum('ijk,ijl->ikl',mat_A, mat_B)\n",
    "matrixMult2d = lambda mat_A, mat_B: np.einsum('ij, jk -> ik', mat_A, mat_B)\n",
    "matrixMult3d = lambda mat_A, mat_B: np.einsum('nmk,nkj->nmj', mat_A, mat_B)\n",
    "matrixMult3d_2d = lambda mat_A, mat_B: np.einsum('ijk,ik->ijk', mat_A, mat_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2_SentimentGenerator(word_vectors, tweets_df=None):\n",
    "    kmeans_params = {\n",
    "        \"n_clusters\": 3,\n",
    "        \"max_iter\": 400\n",
    "        # \"n_init\": 50,\n",
    "        # \"random_state\": 42\n",
    "    }\n",
    "\n",
    "    # First scale data\n",
    "\n",
    "    X_scaled = sk_scale(word_vectors.vectors.astype('double'))\n",
    "\n",
    "    # Then standardize features\n",
    "    X_std = StandardScaler().fit_transform(X_scaled)\n",
    "\n",
    "     # Using PCA to remove columns (features) which have less co-relation\n",
    "    n_clusters = 3\n",
    "    word_vectors_pca = PCA(n_components=n_clusters)\n",
    "    pca_matrix = word_vectors_pca.fit_transform(X_std)\n",
    "\n",
    "    # Initialize dataframe that will hold all calculated output during clustering\n",
    "    words_df = pd.DataFrame({\n",
    "                            'words': word_vectors.index_to_key,\n",
    "                            'vectors': list(X_std),\n",
    "                            'pca_values': list(pca_matrix)\n",
    "                        })\n",
    "\n",
    "    # Define clustering model (KMeans algo)\n",
    "    kmeans_model = KMeans(**kmeans_params)\n",
    "    kmeans_model = kmeans_model.fit(pca_matrix)\n",
    "\n",
    "    # Generate cluster predictions for each sample (word)\n",
    "    X_ = np.stack(words_df.pca_values.to_numpy().ravel()).reshape(-1, n_clusters)\n",
    "    words_df['predicted_cluster'] = kmeans_model.predict(X_)\n",
    "\n",
    "    # Use the model to translate each sample's features into \"model-space\"/\"cluster-space\"\n",
    "    to_cluster_space = lambda x: kmeans_model.transform([x]).flatten()\n",
    "    words_df['cluster_space'] = words_df.pca_values.apply(to_cluster_space)\n",
    "\n",
    "\n",
    "    # #### Prep for Similarity Scoring ####\n",
    "\n",
    "    # ## Declare constants\n",
    "\n",
    "    # # Setup a matrix for comparisons to the coordinates of each cluster's center\n",
    "    # cluster_space_centers = cluster_model.cluster_centers_[:, np.newaxis, :]\n",
    "\n",
    "    # # Define specs about the clustering process\n",
    "    # cluster_sizes = np.bincount(cluster_model.predicted_cluster)\n",
    "    # max_cluster_size = max(cluster_sizes)\n",
    "\n",
    "    # # Boolean mask used for filtering and reshaping\n",
    "    # masking_func = lambda x, y: y < cluster_sizes[x]\n",
    "    # cluster_space_mask = np.ma.fromfunction(masking_func, \n",
    "    #                                             (n_clusters, max_cluster_size), dtype=int)\n",
    "\n",
    "    # # Build a transformation matrix with the cluster-space data\n",
    "    # grouped_cluster_spaces = words_df.groupby(by='predicted_cluster').cluster_space \\\n",
    "    #                                     .apply(lambda x: explodeAndPad(x, cluster_sizes))\n",
    "    # cluster_space_matrix = np.stack(grouped_cluster_spaces.to_numpy().ravel())\n",
    "\n",
    "\n",
    "    # ################## Begin \"scoring pipeline\" ##################\n",
    "\n",
    "\n",
    "    # # Structure query data. Contains every sample separated by its predicted cluster value\n",
    "    # cluster_queries = generateClusterQueries(cluster_space_matrix)\n",
    "\n",
    "    # # Compute the euclidean distance between all clusters and those in each predicted cluster group\n",
    "    # cluster_distances = np.array([ nd_EuclideanDistance(cluster_space_matrix, cluster_queries[i]).T ## DON'T LIKE THIS TRANSPOSE!!!\n",
    "    #                                     for i in range(n_clusters) ])\n",
    "\n",
    "    # # Average the distances acquired above\n",
    "    # avg_cluster_dists = np.array([np.mean(dist, axis=0) for dist in cluster_distances])\n",
    "\n",
    "    # # Determine the std for each cluster\n",
    "    # cluster_distance_stds = np.array([np.std(cluster_distances[i]) for i in range(n_clusters)])\n",
    "\n",
    "    # # Get the distance of each sample to each cluster's center (a very stong indication of similarity)\n",
    "    # predicted_distance_diffs = np.array([nd_EuclideanDistance(cluster_space_centers,\n",
    "    #                                                     cluster_queries[i]).T for i in range(n_clusters)])\n",
    "\n",
    "    # # Generate weights for the confidence level based on the distance between clusters\n",
    "    # transformed_weight = np.dot(avg_cluster_dists,\n",
    "    #                                 scalePredictionDistance(cluster_distances))[:, np.newaxis, :]\n",
    "\n",
    "    # # Build weights based on the spread of the clusters and how \"wide\" of an area they cover\n",
    "    # prediction_weights = cluster_space_matrix * np.reciprocal(cluster_distance_stds)\n",
    "\n",
    "    # # Calculate the model's average error using the distance of each sample and each cluster's average distance\n",
    "    # transformed_error = np.array([nd_EuclideanDistance(cluster_space_matrix,\n",
    "    #                                                     avg_cluster_dists[:, np.newaxis, :]).T\n",
    "    #                                 for i in range(n_clusters)])\n",
    "\n",
    "    # # Build an error score for the model's transformation\n",
    "    # prediction_error = np.divide(predicted_distance_diffs, (transformed_error * transformed_weight))\n",
    "\n",
    "    # # Finally, combine the overall predictions' error and weight to build a confidence score\n",
    "    # prediction_confidence = np.reciprocal((prediction_error))  * prediction_weights\n",
    "\n",
    "    # # Store the relative similarities between each cluster\n",
    "    # global_cluster_similarity = np.divide(1, avg_cluster_dists, out=np.zeros_like(avg_cluster_dists),\n",
    "    #                                     where=avg_cluster_dists!=0)\n",
    "\n",
    "    # # Also maintain a similarity score with each cluster for every sample\n",
    "    # sample_similarity = np.divide(1, cluster_distances, out=np.zeros_like(cluster_distances),\n",
    "    #                                     where=cluster_distances!=0)\n",
    "    \n",
    "\n",
    "    # ################## End \"scoring pipeline\" ##################\n",
    "\n",
    "    # words_df.update({\n",
    "    #     'confidence_score': prediction_confidence,\n",
    "    #     'global_similarity': global_cluster_similarity,\n",
    "    #     'sample_similarity': sample_similarity\n",
    "    # })\n",
    "\n",
    "    # return words_df, cluster_model\n",
    "\n",
    "    return words_df, kmeans_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_predictions, cluster_model = v2_SentimentGenerator(WORD_VECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_predictions.predicted_cluster.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            ##### Define independent constants #####\n",
    "\n",
    "# Cluster data specs/sizes\n",
    "cluster_sample_sizes = np.bincount(word_predictions.predicted_cluster)\n",
    "n_clusters, n_features = cluster_model.cluster_centers_.shape\n",
    "n_samples = sum(cluster_sample_sizes)\n",
    "smallest_cluster, largest_cluster = min(cluster_sample_sizes), max(cluster_sample_sizes)\n",
    "cluster_space_dims = (n_clusters, largest_cluster, n_features)\n",
    "\n",
    "\n",
    "# Setup a matrix for comparisons to the coordinates of each cluster's center\n",
    "cluster_space_centers = cluster_model.cluster_centers_[:, np.newaxis, :]\n",
    "\n",
    "\n",
    "# Boolean mask used for filtering and reshaping\n",
    "masking_func = lambda x, y: y < cluster_sample_sizes[x]\n",
    "cluster_space_mask = np.ma.fromfunction(masking_func, (n_clusters, largest_cluster), dtype=int)\n",
    "\n",
    "\n",
    "# Utility function to undo the transformation necessary for scaling 3d matricies\n",
    "restoreMat3d = lambda mat: mat.reshape(*cluster_space_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build a transformation matrix with the cluster-space data \n",
    "\n",
    "# Group the data by the predicited cluster value\n",
    "cluster_groups = word_predictions.groupby(by='predicted_cluster').cluster_space \\\n",
    "                                    .apply(lambda x: explodeAndPad(x, cluster_sample_sizes))\n",
    "\n",
    "\n",
    "# Convert the grouped data into a usable numpy array\n",
    "cluster_space_matrix = np.stack(cluster_groups.to_numpy().ravel())\n",
    "\n",
    "\n",
    "# Structure query data. Contains every sample separated by its predicted cluster value\n",
    "cluster_queries = generateClusterQueries(cluster_space_matrix)\n",
    "\n",
    "\n",
    "# Compute the euclidean distance between all samples and the entire space\n",
    "sample_distances = calcDistance(cluster_queries, cluster_space_matrix)\n",
    "\n",
    "\n",
    "description = f'''\n",
    "cluster_groups:     {cluster_groups.shape}\n",
    "cluster_space_mat   {cluster_space_matrix.shape}\n",
    "cluster_queries:    {cluster_queries.shape}\n",
    "sample_distances:   {sample_distances.shape}\n",
    "'''\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                ########## Basic Stats ##########\n",
    "\n",
    "### Averages ###\n",
    "\n",
    "# Average the distances acquired above to produce approximate cluster centers\n",
    "avg_observed_distances = np.mean(sample_distances, axis=1)\n",
    "\n",
    "# Using the modeled cluster space, find the average \"distances\" per cluster (all will be [3,3])\n",
    "avg_model_distances = np.mean(cluster_space_matrix, axis=1)\n",
    "\n",
    "\n",
    "### Standard Deviation ###\n",
    "\n",
    "# Determine the std for each cluster\n",
    "observed_distances_std = np.std(sample_distances, axis=1)\n",
    "\n",
    "# Compute the std for each cluster using the cluster-space\n",
    "model_distances_std = np.std(cluster_space_matrix, axis=1)\n",
    "\n",
    "\n",
    "### Variance ###\n",
    "\n",
    "# Determine the var of each cluster\n",
    "observed_distances_var = np.var(sample_distances, axis=1)\n",
    "\n",
    "# Determine the var of each cluster\n",
    "model_distances_var = np.var(cluster_space_matrix, axis=1)\n",
    "\n",
    "\n",
    "description = f'''\n",
    "Averages:\n",
    "    observed/sample intra-population distances:     {avg_observed_distances.shape}\n",
    "    model/calculated intra-population distances:    {avg_model_distances.shape}\n",
    "\n",
    "Standard Devs:\n",
    "    observed/sample intra-population distances:     {observed_distances_std.shape}\n",
    "    model/calculated intra-population distances:    {model_distances_std.shape}\n",
    "\n",
    "Variance:\n",
    "    observed/sample intra-population distances:     {observed_distances_var.shape}\n",
    "    model/calculated intra-population distances:    {model_distances_var.shape}\n",
    "'''\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                    ######## Differencing ########\n",
    "\n",
    "\n",
    "# Get the distance of each sample to each cluster's center (a very stong indication of similarity)\n",
    "model_center_diffs = calcDistance(cluster_queries, cluster_space_centers)\n",
    "\n",
    "# Get the distance of every sample from the average of their combined distances\n",
    "observed_center_diffs = calcDistance(cluster_queries, np.expand_dims(avg_model_distances, axis=1))\n",
    "\n",
    "# Calculate the difference in average distances between the observed/calculated version and the one provied \n",
    "# by the clustering model\n",
    "avg_distance_diffs = calcDistance(avg_model_distances, avg_observed_distances)\n",
    "\n",
    "\n",
    "\n",
    "description = f'''\n",
    "model_center_diffs:     {model_center_diffs.shape}\n",
    "observed_center_diffs:  {observed_center_diffs.shape}              \n",
    "avg_distance_diffs      {avg_distance_diffs.shape}\n",
    "'''\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up matrices for the following calculations\n",
    "\n",
    "# Invert the sample distance's to each cluster\n",
    "center_similarity = np.linalg.pinv(model_center_diffs)\n",
    "\n",
    "\n",
    "# Utilize KMeans' intertia property\n",
    "samples_distribution = [(x/n_samples) for x in cluster_sample_sizes]\n",
    "\n",
    "cluster_distance_metric = [x*cluster_model.inertia_ for x in samples_distribution]\n",
    "\n",
    "\n",
    "# Build an inertia value by tracing KMeans' implementation\n",
    "samples_df = np.stack(word_predictions.pca_values.to_numpy().ravel())\n",
    "shortest_squared_distances = np.square(np.apply_along_axis(min, axis=1, arr=samples_df))\n",
    "observed_inertia = np.sum(shortest_squared_distances)\n",
    "\n",
    "observed_distance_metric = [x*observed_inertia for x in samples_distribution]\n",
    "\n",
    "\n",
    "description = f'''\n",
    "center_similarity:          {center_similarity.shape}\n",
    "model_intertia:             {cluster_model.inertia_}\n",
    "cluster_distance_metric:    {cluster_distance_metric}\n",
    "observed_inertia:           {observed_inertia}\n",
    "observed_distance_metric:   {observed_distance_metric}\n",
    "'''\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Define distance/observable\n",
    "\n",
    "# Calculate the model's weights per-feature & per-cluster using the distance of each sample and each cluster's location\n",
    "\n",
    "inv_observed_dists_std = np.expand_dims(np.linalg.pinv(observed_distances_std), axis=1)\n",
    "observed_distance_weights = matrixMult3d(sample_distances, inv_observed_dists_std)\n",
    "\n",
    "\n",
    "# Determine the distance between the \"sample-space\" and the \"cluster-space\"\n",
    "distances_diff = calcDistance(sample_distances, cluster_space_matrix)\n",
    "\n",
    "inv_observed_dists_var = np.expand_dims(np.linalg.pinv(observed_distances_var), axis=1)\n",
    "sample_distance_error_1 = matrixMult3d(observed_center_diffs, inv_observed_dists_var)\n",
    "sample_distance_error_2 = matrixDot3d(sample_distance_error_1, observed_distance_weights)\n",
    "sample_distance_error_1 = matrixMult3d(np.moveaxis(center_similarity, 2, 1), sample_distance_error_2)\n",
    "sample_distance_error = calcDistance(distances_diff, sample_distance_error_1)\n",
    "\n",
    "\n",
    "# # Build an error score for the model's transformation\n",
    "inv_observed_dists_var = np.expand_dims(np.linalg.pinv(observed_distances_var), axis=1)\n",
    "weighted_observed_dists_err = matrixMult3d(observed_distance_weights, np.expand_dims(avg_distance_diffs, 1))\n",
    "weighted_observed_dists_err += matrixMult3d(sample_distances, sample_distance_error.reshape(3, 3, -1))\n",
    "\n",
    "\n",
    "description = f'''\n",
    "observed_distance_weights:      {observed_distance_weights.shape}\n",
    "distances_diff:                 {distances_diff.shape}\n",
    "sample_distance_error:          {sample_distance_error.shape}\n",
    "weighted_observed_dists_err:    {weighted_observed_dists_err.shape}\n",
    "'''\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Prediction weights ###\n",
    "\n",
    "# Build weights based on the spread of the clusters and how \"wide\" of an area they cover\n",
    "inv_model_center_vars = np.expand_dims(np.linalg.pinv(model_distances_var), axis=1)\n",
    "\n",
    "tmp_global_weight = matrixMult3d(model_center_diffs, inv_model_center_vars)\n",
    "global_prediction_weight = matrixDot3d(tmp_global_weight, weighted_observed_dists_err)\n",
    "\n",
    "\n",
    "inv_center_similarity = np.linalg.pinv(center_similarity)\n",
    "prediction_weights_1 = matrixMult3d(center_similarity, inv_model_center_vars)\n",
    "prediction_weights_2 = matrixMult3d(inv_center_similarity, np.expand_dims(avg_distance_diffs, 1))\n",
    "prediction_weights = matrixDot3d(prediction_weights_1, prediction_weights_2.reshape(3, 3, -1))\n",
    "\n",
    "### Prediction errors ###\n",
    "\n",
    "# Get the dot product of the model's cluster's center and the each samples distance from the clusters\n",
    "t1 = matrixDot3d(cluster_space_centers, model_center_diffs)\n",
    "t2 = matrixDot3d()\n",
    "\n",
    "### Weighted errors ###\n",
    "\n",
    "# Finally, combine the overall predictions' error and weight to build a confidence score\n",
    "reciprocal_prediction_err = np.reciprocal(prediction_error)\n",
    "\n",
    "\n",
    "### Confidence ###\n",
    "\n",
    "prediction_confidence = matrixDot3d(reciprocal_prediction_err.T, prediction_error)\n",
    "\n",
    "\n",
    "description = f'''\n",
    "global_prediction_weight:   {global_prediction_weight.shape}\n",
    "prediction_weights:         {prediction_weights.shape}\n",
    "prediction_error:           {prediction_error.shape}\n",
    "reciprocal_prediction_err:  {reciprocal_prediction_err.shape}\n",
    "prediction_confidence:      {prediction_confidence.shape}\n",
    "'''\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the relative similarities between each cluster (global)\n",
    "\n",
    "inv_avg_distance_err = np.linalg.pinv(avg_distance_diffs)\n",
    "\n",
    "\n",
    "\n",
    "global_cluster_similarity = matrixMult2d(inv_avg_distance_err, weighted_distances_err)\n",
    "\n",
    "\n",
    "\n",
    "# Maintain a similarity score with each cluster for every sample (narrow)\n",
    "\n",
    "inv_distance_stds = np.linalg.pinv(observed_distances_std)\n",
    "\n",
    "sample_similarity = matrixMult3d_2d(sample_distances, inv_distance_stds)\n",
    "sample_similarity += matrixMult2d(inv_distance_stds, distances_diff)[...,np.newaxis]\n",
    "\n",
    "\n",
    "distances_diff.shape, global_cluster_similarity.shape, sample_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'confidence_score': prediction_confidence,\n",
    "    'global_similarity': global_cluster_similarity,\n",
    "    'sample_similarity': sample_similarity\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in scores.items():\n",
    "    print(k)\n",
    "    print(v.shape)\n",
    "    print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cluster_queries[0]\n",
    "\n",
    "nd_EuclideanDistance(cluster_space_matrix, np.squeeze(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cluster_queries[1]\n",
    "\n",
    "nd_EuclideanDistance(cluster_space_matrix, np.squeeze(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1_cluster_df[['cluster_similarity', 'confidence_scores']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.stack(opt1_cluster_df['confidence_scores'].to_numpy().ravel()).T[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_confidence = np.stack(opt1_cluster_df['confidence_scores'].to_numpy().ravel())\n",
    "sns.kdeplot(data=raw_confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(arr):\n",
    "    for a in arr.T:\n",
    "        print(stats.describe(a))\n",
    "        # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "mm_scaled = mm_scaler.fit_transform(np.stack(out.confidence_score.to_numpy().ravel()))\n",
    "sns.kdeplot(data=(mm_scaled))\n",
    "print_stats(mm_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "std_scaled = std_scaler.fit_transform(np.stack(out.confidence_score.to_numpy().ravel()))\n",
    "sns.kdeplot(data=std_scaled)\n",
    "print_stats(std_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rob_scaler = RobustScaler()\n",
    "rob_scaled = rob_scaler.fit_transform(np.stack(out.confidence_score.to_numpy().ravel()))\n",
    "sns.kdeplot(data=rob_scaled)\n",
    "print_stats(rob_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_scaler = MaxAbsScaler()\n",
    "abs_scaled = abs_scaler.fit_transform(np.stack(out.confidence_score.to_numpy().ravel()))\n",
    "sns.kdeplot(data=abs_scaled)\n",
    "print_stats(abs_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('robust', MaxAbsScaler()), ('minmax', MinMaxScaler(feature_range=(-1,1)))])\n",
    "pipe_scaled = pipe.fit_transform(np.stack(out.confidence_score.to_numpy().ravel()))\n",
    "sns.kdeplot(data=pipe_scaled)\n",
    "print_stats(pipe_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=(mm_scaled))\n",
    "print_stats(mm_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_lookup_df = words_clusters_df[['cluster_scores', 'cluster_space']]\n",
    "# grouping_features = ['cluster_scores', 'cluster_space']\n",
    "scoring_dimensions = ['cluster_space', 'cluster_similarity', 'confidence_score']\n",
    "words_clusters_df = out[['words', 'predicted_cluster']+scoring_dimensions]\n",
    "words_clusters_df = words_clusters_df.set_index('words')\n",
    "\n",
    "def clusterGrouping(row):\n",
    "    words = row.clean_tweet_words\n",
    "    # words_search = np.stack(words_clusters_df.loc[words][scoring_dimensions].to_numpy().ravel()).reshape(3, -1, 3)\n",
    "    words_search = words_clusters_df.loc[words][scoring_dimensions]\n",
    "    distances, similarities = np.stack(words_search[['cluster_space', 'cluster_scores']].to_numpy().ravel()).reshape(2, -1, 3)\n",
    "    confidences = words_search['confidence_score'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    weighted_distances = similarities * distances * confidences\n",
    "\n",
    "    closest_clusters = np.argmin(weighted_distances, axis=1)\n",
    "\n",
    "    agg_weights = [[] for _ in range(3)]\n",
    "    for cluster, weights in zip(closest_clusters, weighted_distances):\n",
    "        agg_weights[cluster].append(weights[cluster])\n",
    "\n",
    "    avg_weights = [np.mean(closest_weights) if closest_weights else np.nan for closest_weights in agg_weights]\n",
    "    closest_cluster = np.argmin(avg_weights)\n",
    "\n",
    "    return closest_cluster, avg_weights[closest_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_words = FILTERED_TWEETS.iloc[1].clean_tweet_words\n",
    "sample_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_search = words_clusters_df.loc[sample_words][scoring_dimensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = FILTERED_TWEETS.iloc[1].clean_tweet_words\n",
    "# words_search = np.stack(words_clusters_df.loc[words][scoring_dimensions].to_numpy().ravel()).reshape(3, -1, 3)\n",
    "words_search = words_clusters_df.loc[words][scoring_dimensions]\n",
    "distances, similarities = np.stack(words_search[['cluster_space', 'cluster_scores']].to_numpy().ravel()).reshape(2, -1, 3)\n",
    "confidences = words_search['confidence_score'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "weighted_distances = similarities * distances * confidences\n",
    "\n",
    "closest_clusters = np.argmax(weighted_distances, axis=1)\n",
    "\n",
    "agg_weights = [[] for _ in range(3)]\n",
    "for cluster, weights in zip(closest_clusters, weighted_distances):\n",
    "    agg_weights[cluster].append(weights[cluster])\n",
    "\n",
    "avg_weights = [np.mean(closest_weights) for closest_weights in agg_weights if closest_weights]\n",
    "closest_cluster = np.argmax(avg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def kmeans_clustering(Y_sklearn, fitted):\n",
    "#     \"\"\"\n",
    "#     This function will predict clusters on training set and plot the visuals of clusters as well.\n",
    "#     \"\"\"\n",
    "\n",
    "#     plt.scatter(Y_sklearn[:, 0], Y_sklearn[:, 1],c=prediction ,s=50, cmap='viridis') # Plotting scatter plot \n",
    "#     centers2 = fitted.cluster_centers_ # It will give best possible coordinates of cluster center after fitting k-means\n",
    "#     plt.scatter(centers2[:, 0], centers2[:, 1],c='black', s=300, alpha=0.6);\n",
    "#     # As this can be seen from the figure, there is an outlier as well.\n",
    "    \n",
    "# kmeans_clustering(Y_sklearn, fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_cluster(ngrams, X_std, prediction, n_feats):\n",
    "    # Get unique labels, in this case {0,1}\n",
    "    labels = np.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = np.where(prediction==label) # Get indices for each feature corresponding to each cluster.        \n",
    "        x_means = np.mean(X_std[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        # features = n_grams_to_use\n",
    "        best_features = [(ngrams[i], x_means[i]) for i in sorted_means] # Retrieve corresponding best features to that of best scores.\n",
    "        Df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(Df) # append both the Dataframes to a list\n",
    "    return dfs\n",
    "\n",
    "dfs = get_top_features_cluster(wv.index_to_key, X_std, prediction, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[:25][0]) # Get top 25 rows of 1st Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[:25][1]) # Get top 25 rows of 2nd Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[:25][2]) # Get top 25 rows of 3nd Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(dfs):\n",
    "    \"\"\"\n",
    "    This function will print combined bar graphs for all the possible clusters.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(14,12))\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.set_title(\"Cluster: \"+ str(i), fontsize = 14)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.score, align='center', color='#40826d')\n",
    "        yticks = ax.set_yticklabels(df.features)\n",
    "    plt.show();\n",
    "\n",
    "plot_features(dfs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d883baf2d2c020187d16fb74e1bc85e676b385dd78044a08a209b4abcafece"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
