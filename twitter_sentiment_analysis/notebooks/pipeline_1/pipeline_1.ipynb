{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Pipeline #1\n",
    "*Refer to `notebooks/README.md` for an explanation of the various pipelines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from shared_imports import (pd,\n",
    "                            importData,\n",
    "                            ControlSignal, CONTROL_ACTIONS, CONTROL_FLAGS, processSignals, \n",
    "                            buildConfig, mergeDicts,\n",
    "                            Grapher)\n",
    "\n",
    "from utils.preprocessing import *\n",
    "from utils.model_assessment import *\n",
    "from utils.sentiment_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:31.354913Z",
     "start_time": "2022-04-07T22:47:31.043798Z"
    }
   },
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "# Data manipulation\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:33.893035Z",
     "start_time": "2022-04-07T22:47:32.167203Z"
    }
   },
   "outputs": [],
   "source": [
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `extract` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:47.327270Z",
     "start_time": "2022-04-07T22:47:47.295179Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract(import_path, import_dest, **kwargs):\n",
    "    signals=[]\n",
    "    print(f'Importing data from \"{import_path}\"...')\n",
    "\n",
    "    _, raw_tweets_df = importData(import_loc=import_path, import_protocol=kwargs.get('import_path_protocol'),\n",
    "                                  local_dest=import_dest, signals=signals, kwargs=kwargs)\n",
    "\n",
    "    # Look for existing models/datasets\n",
    "    existing_models = {}\n",
    "    if (word_vec_path := kwargs.get('word_vec')):\n",
    "        existing_models['word_vec'] = Word2Vec.load(word_vec_path).wv\n",
    "    if (kmeans_path := kwargs.get('kmeans')):\n",
    "        existing_models['kmeans'] = load(kmeans_path)\n",
    "    if (embeddings_path := kwargs.get('embeddings')):\n",
    "        existing_models['embeddings'] = pd.read_csv(embeddings_path)\n",
    "    if (vectorizer_path := kwargs.get('vectorizer')):\n",
    "        existing_models['vectorizer'] = load(vectorizer_path)\n",
    "\n",
    "    # Predictive models\n",
    "    if (linear_svc_path := kwargs.get('linear_svc')):\n",
    "        existing_models['linear_svc'] = (load(linear_svc_path), {}) # Place holder for performance metrics\n",
    "    if (multi_nb_path := kwargs.get('multi_nb')):\n",
    "        existing_models['multi_nb'] = (load(multi_nb_path), {}) # Place holder for performance metrics\n",
    "\n",
    "    return signals, (raw_tweets_df, existing_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `transform` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:52.399831Z",
     "start_time": "2022-04-07T22:47:52.291560Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform(raw_tweets_df, sentiment_map, column_mappings={}, \n",
    "                                filter_words=[], existing_models={}, **kwargs):\n",
    "    signals = []\n",
    "    _, clean_datasets = cleanAndFilter(raw_tweets_df=raw_tweets_df, \n",
    "                                                    column_mappings=column_mappings, \n",
    "                                                    filter_words=filter_words, \n",
    "                                                    signals=signals,\n",
    "                                                    **kwargs)\n",
    "    clean_tweets, filtered_tweets = clean_datasets\n",
    "\n",
    "    # Gather/build word vector model\n",
    "    if (existing_word_vec := existing_models.get('word_vec')):\n",
    "        word_vectors = existing_word_vec\n",
    "    else:\n",
    "        _, word_vectors = buildWordVectors(clean_tweets, signals=signals, **kwargs)\n",
    "        existing_models['word_vec'] = word_vectors\n",
    "    \n",
    "    # Gather/build clustering model\n",
    "    if (existing_kmeans := existing_models.get('kmeans')):\n",
    "        cluster_model = existing_kmeans\n",
    "    else:\n",
    "        # Build KMeans model to cluster words into positive, negative, and neutral clusters\n",
    "        if (kmeans_params := kwargs.get('kmeans_params')):\n",
    "            cluster_model = KMeans(**kmeans_params)\n",
    "            existing_models['kmeans'] = cluster_model\n",
    "        else:\n",
    "            signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.MISSING_NECCESSARY, \n",
    "                                            'Need parameters for the KMeans clustering algorithm.'))\n",
    "            return signals, None\n",
    "\n",
    "        # Train the cluster model\n",
    "        if (kmeans_train := kwargs.get('kmeans_train')):    \n",
    "            cluster_model = cluster_model.fit(X=word_vectors.vectors.astype('double'), **kmeans_train)\n",
    "        else:\n",
    "            cluster_model = cluster_model.fit(X=word_vectors.vectors.astype('double'))\n",
    "                        \n",
    "    ############# Get User Input for Sentiment Assignment ###############\n",
    "    _, cluster_sentiment_defs = setClusterSentiment(vectors=word_vectors, \n",
    "                                                    model=cluster_model, \n",
    "                                                    sentiment_mapping=sentiment_map, \n",
    "                                                    signals=signals,\n",
    "                                                    display_terms=kwargs.get('display_terms'))\n",
    "    if not cluster_sentiment_defs:\n",
    "        return signals, None\n",
    "    #####################################################################\n",
    "    \n",
    "    print('\\nApplying sentiment mapping...')\n",
    "\n",
    "    # Gather/generate word embeddings\n",
    "    if (existing_embeddings := existing_models.get('embeddings')):\n",
    "        words_df = existing_embeddings\n",
    "    else:\n",
    "        words_df = buildWordEmbeddings(word_vectors=word_vectors, \n",
    "                                        model=cluster_model, \n",
    "                                        sentiment_defs=cluster_sentiment_defs, \n",
    "                                        sentiment_map=sentiment_map)\n",
    "        existing_models['embeddings'] = words_df\n",
    "        \n",
    "    # Get the sentiment for the entire tweet\n",
    "    if (threshold := kwargs.get('sentiment_threshold')):\n",
    "        \n",
    "        words_cluster_dict = dict(zip(words_df.words, words_df.cluster_value))    \n",
    "\n",
    "        def getSentiment(row):\n",
    "            words_list = row['clean_tweet_words']\n",
    "            total = sum(int(words_cluster_dict.get(word, 0)) for word in words_list)\n",
    "            avg = total / len(words_list)\n",
    "            return -1 if (avg < -threshold) else 1 if (avg > threshold) else 0\n",
    "\n",
    "        # Add sentiment column (integer values)\n",
    "        filtered_tweets[\"sentiment_val\"] = filtered_tweets.apply(getSentiment, axis=1)\n",
    "        # Map integer sentiment to word value\n",
    "        filtered_tweets[\"sentiment\"] = filtered_tweets[\"sentiment_val\"].map(sentiment_map)\n",
    "    else:\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.MISSING_NECCESSARY, \n",
    "                                        'Need sentiment threshold parameter to assign sentiment values.'))\n",
    "        return signals, None\n",
    "\n",
    "    # Confirm sentiment distribution with user\n",
    "    if not peekSentimentDistrib(filtered_tweets):\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.USER_INPUT, \n",
    "                                        'Distribution was unsatisfactory.'))\n",
    "        return signals, None\n",
    "    \n",
    "    return signals, (filtered_tweets, words_df, cluster_sentiment_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:59.009834Z",
     "start_time": "2022-04-07T22:47:58.996891Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def model(sentiment_df, existing_models={}, **kwargs):\n",
    "    signals = []\n",
    "    # Convert each sentiment to df (no need to worry about memory crash, small dataset)\n",
    "    pos_df = sentiment_df[sentiment_df[\"sentiment\"]==\"positive\"]\n",
    "    neg_df = sentiment_df[sentiment_df[\"sentiment\"]==\"negative\"]\n",
    "    neu_df = sentiment_df[sentiment_df[\"sentiment\"]==\"neutral\"]\n",
    "\n",
    "    # Combine all sentiments in one df\n",
    "    sentiments_df_list = [pos_df, neg_df, neu_df] \n",
    "    agg_sentiment_df = pd.concat(sentiments_df_list)\n",
    "\n",
    "    # Split the data to training, testing, and validation data \n",
    "    test_size = 0.25 # default value provided by scikit-learn\n",
    "    if (config_test_size := kwargs.get('test_size')):\n",
    "        test_size = config_test_size\n",
    "    else:\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.INFO, CONTROL_FLAGS.MISSING_OPTIONAL_ARGS, \n",
    "                                        'No test size provided for model training. Will use default.'))\n",
    "    \n",
    "    train_test_df, _ = train_test_split(agg_sentiment_df, test_size=test_size, random_state=10)\n",
    "\n",
    "    X = train_test_df['clean_tweet']\n",
    "    y = train_test_df['sentiment_val']\n",
    "\n",
    "    # Split the dataset set into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Instantiate TfidfVectorizer \n",
    "    if (existing_vectorizer := existing_models.get('vectorizer')):\n",
    "        vectorizer = existing_vectorizer\n",
    "    else: \n",
    "        if (vectorizer_params := kwargs.get('vectorizer_params')):\n",
    "            vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "        else:\n",
    "            signals.append(ControlSignal(CONTROL_ACTIONS.INFO, CONTROL_FLAGS.MISSING_OPTIONAL_ARGS, \n",
    "                                        'No provided argument for `TfidfVectorizer` object. Will use defaults.'))\n",
    "            vectorizer = TfidfVectorizer(min_df=3, stop_words='english')\n",
    "\n",
    "    # Fit vectorizer\n",
    "    X_train_tf = vectorizer.fit_transform(X_train.reset_index()[\"clean_tweet\"]).toarray()\n",
    "    X_test_tf = vectorizer.transform(X_test.reset_index()[\"clean_tweet\"]).toarray()\n",
    "\n",
    "    # Store features from the vectors\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a dict of Sentiment_val: sentiments to use with the confusion matrix\n",
    "    sentiment_id_df = agg_sentiment_df[['sentiment', 'sentiment_val']].drop_duplicates() \\\n",
    "                                                                        .sort_values('sentiment_val')\n",
    "    sentiment_to_id = dict(sentiment_id_df.values)\n",
    "\n",
    "    ## LinearSVC ##\n",
    "\n",
    "    if (existing_linear_svc := existing_models.get('linear_svc')):\n",
    "        # (NOTE: Must use same vectorizer from its fitting)\n",
    "        linearSVC = existing_linear_svc\n",
    "    else:\n",
    "        # Instantiate the model\n",
    "        if (linearSVC_params := kwargs.get('linear_svc_params')):\n",
    "            linearSVC = LinearSVC(**linearSVC_params)\n",
    "        else:\n",
    "            signals.append(ControlSignal(CONTROL_ACTIONS.INFO, CONTROL_FLAGS.MISSING_OPTIONAL_ARGS, \n",
    "                                        'No provided argument for `LinearSVC` object. Will use defaults.'))\n",
    "            linearSVC = LinearSVC()\n",
    "\n",
    "        # Fit the model\n",
    "        linearSVC.fit(X_train_tf, y_train)\n",
    "\n",
    "    svc_performance = performanceSummary(model=linearSVC, X_test=X_test_tf, y_test=y_test)\n",
    "\n",
    "    ## MultinomialNB ##\n",
    "\n",
    "    if (existing_multi_nb := existing_models.get('multi_nb')):\n",
    "        # (NOTE: Must use same vectorizer from its fitting)\n",
    "        multiNB = existing_multi_nb\n",
    "    else:\n",
    "        # Instantiate the model\n",
    "        if (multiNB_params := kwargs.get('multi_nb_params')):\n",
    "            multiNB = MultinomialNB(**multiNB_params)\n",
    "        else:\n",
    "            signals.append(ControlSignal(CONTROL_ACTIONS.INFO, CONTROL_FLAGS.MISSING_OPTIONAL_ARGS, \n",
    "                                        'No provided argument for `MultinomialNB` object. Will use defaults.'))\n",
    "            multiNB = MultinomialNB()\n",
    "\n",
    "        # Fit the model\n",
    "        multiNB.fit(X_train_tf, y_train)\n",
    "\n",
    "    nb_performance = performanceSummary(model=multiNB, X_test=X_test_tf, y_test=y_test)\n",
    "\n",
    "    return signals, {\n",
    "                'linear_svc': (linearSVC, svc_performance),\n",
    "                'multi_nb': (multiNB, nb_performance),\n",
    "                'vectorizer': vectorizer,\n",
    "                'features': feature_names,\n",
    "                'sentiment_id': sentiment_id_df,\n",
    "                'sentiment_to': sentiment_to_id\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(transform_df, built_models={}, destinations={}, config=None):\n",
    "    signals = []\n",
    "    # Export the sentiment dataframe\n",
    "    if (transform_dest := destinations.get('transform')):\n",
    "        transform_df.to_csv(transform_dest)\n",
    "\n",
    "    if (config_loc := destinations.get('config')):\n",
    "        with open(config_loc, 'w') as f:\n",
    "            json.dump(config, f)\n",
    "    \n",
    "    # Pair up models with their export destination \n",
    "    model_exports = { name: path for name, path in destinations.items() \n",
    "                                            if (name in built_models and built_models[name] is not None) }\n",
    "    for name, path in model_exports.items():\n",
    "        if name == 'word_vec':\n",
    "            built_models[name].save(path)\n",
    "        elif name == 'embeddings':\n",
    "            built_models[name].to_csv(path)\n",
    "        elif isinstance(built_models[name], tuple): # Check for model with associated performance metrics\n",
    "            dump(built_models[name][0], path)\n",
    "        else:\n",
    "            dump(built_models[name], path)\n",
    "            \n",
    "    # Save current notebook for import\n",
    "    if (notebook_dest := destinations.get('notebook')):\n",
    "        \n",
    "        !jupyter nbconvert --output {notebook_dest} --to script pipeline_1.ipynb\n",
    "\n",
    "        # Get rid of excess\n",
    "        with open(notebook_dest + '.py', 'r+') as fp:\n",
    "            lines = fp.readlines()\n",
    "            fp.seek(0)\n",
    "            fp.truncate()\n",
    "            cell_markers = set([])\n",
    "            term_index = len(lines) - 1\n",
    "            for i, line in enumerate(lines):\n",
    "                if '# Execute `pipeline`' in line:\n",
    "                    term_index = i\n",
    "                    break\n",
    "                elif '# In[' in line:\n",
    "                    cell_markers.add(i)\n",
    "\n",
    "            fp.writelines([l for i, l in enumerate(lines[:term_index]) if i not in cell_markers])\n",
    "    return signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `pipeline` function\n",
    "Combines the above processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:48:17.303608Z",
     "start_time": "2022-04-07T22:48:17.298387Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def pipeline1(default_configs, user_configs=None, extract_args={}, transform_args={}, model_args={}, load_args={}, log_level=None):\n",
    "    # Parse arguments\n",
    "    parsing_signals, valid_params = buildConfig(dflt_configs=default_configs, usr_configs=user_configs, \n",
    "                                                    extract_config=extract_args, transform_config=transform_args, \n",
    "                                                    model_config=model_args, load_config=load_args)\n",
    "    processSignals(signals=parsing_signals, log_level=log_level) # Process error/info signals\n",
    "    extract_params, transform_params, model_params, load_params = itemgetter(*['EXTRACT', 'TRANSFORM', 'MODEL', 'LOAD'])(valid_params)\n",
    "\n",
    "    # Store run-specific information\n",
    "    execution_config = defaultdict(dict)\n",
    "    print('\\n--- Executing Pipeline 1. ---\\n')\n",
    "\n",
    "    ## Extract (import)\n",
    "    print('[Pipeline 1] Stage 1: Extracting...')\n",
    "    extract_signals, extracted_data = extract(**extract_params)\n",
    "    processSignals(signals=extract_signals, generated_files=load_params, log_level=log_level) # Process error/info signals\n",
    "    raw_tweets_df, imported_models = extracted_data\n",
    "    print('[Pipeline 1] Completed Stage 1.', end='\\n\\n')\n",
    "\n",
    "    ## Transform\n",
    "    print('[Pipeline 1] Stage 2: Transforming...')\n",
    "    transform_signals, transform_data = transform(raw_tweets_df=raw_tweets_df, existing_models=imported_models, \n",
    "                                                    **transform_params)\n",
    "    processSignals(signals=transform_signals, generated_files=load_params, log_level=log_level) # Process error/info signals\n",
    "    tweet_sentiment_df, word_vecs, sentiment_defs = transform_data\n",
    "\n",
    "    # Store sentiment encodings\n",
    "    execution_config['MODEL']['sentiment_vals'] = {\n",
    "        'value_mapping': transform_params['sentiment_map'],\n",
    "        'cluster_mapping': sentiment_defs\n",
    "    }\n",
    "    print('[Pipeline 1] Completed Stage 2.', end='\\n\\n')\n",
    "\n",
    "    ## Modeling\n",
    "    if model_params.get('build_models'):\n",
    "        print('[Pipeline 1] Stage 2.5: Modeling...')\n",
    "        model_signals, model_data = model(sentiment_df=tweet_sentiment_df, existing_models=imported_models, \n",
    "                                            **model_params)\n",
    "        processSignals(signals=model_signals, generated_files=load_params, log_level=log_level) # Process error/info signals\n",
    "\n",
    "        imported_models.update(model_data) # Update previously imported models\n",
    "        print('[Pipeline 1] Completed Stage 2.5.', end='\\n\\n')\n",
    "        \n",
    "    ## Loading (export)\n",
    "    print('[Pipeline 1] Stage 3: Loading...')\n",
    "    mergeDicts(execution_config, valid_params)\n",
    "    load_signals = load(transform_df=tweet_sentiment_df, built_models=imported_models, \n",
    "                            destinations=load_params, config=execution_config)\n",
    "    processSignals(signals=load_signals, generated_files=load_params, log_level=log_level) # Process error/info signals\n",
    "    print('[Pipeline 1] Completed Stage 3.', end='\\n\\n')\n",
    "    print('[Pipeline 1] <done>')\n",
    "    \n",
    "    return tweet_sentiment_df, word_vecs, imported_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:52:33.738405Z",
     "start_time": "2022-04-07T22:50:56.370146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Executing Pipeline 1. ---\n",
      "\n",
      "[Pipeline 1] Stage 1: Extracting...\n",
      "Importing data from \"../../data/slava_ukraini_tweets_v11.csv\"...\n",
      "Index(['id', 'user_name', 'user_location', 'user_description', 'user_created',\n",
      "       'user_followers', 'user_friends', 'user_favourites', 'user_verified',\n",
      "       'date', 'text', 'hashtags', 'source', 'retweets', 'favorites',\n",
      "       'is_retweet'],\n",
      "      dtype='object')\n",
      "\n",
      "[Pipeline 1] Completed Stage 1.\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petergish/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3405: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "USER_CONFIG_PATH = './config/user_config.json'\n",
    "DFLT_CONFIG_PATH = './config/default_config.json'\n",
    "LOG_LEVEL = CONTROL_ACTIONS.WARNING\n",
    "\n",
    "sentiment_df, word_vecs, models = pipeline1(default_configs=DFLT_CONFIG_PATH, \n",
    "                                            user_configs=USER_CONFIG_PATH,\n",
    "                                            log_level=LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pie chart of word sentiment distribution\n",
    "Grapher.graphWordDist(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pie chart of tweet sentiment distribution\n",
    "Grapher.graphTweetDist(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d883baf2d2c020187d16fb74e1bc85e676b385dd78044a08a209b4abcafece"
  },
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
