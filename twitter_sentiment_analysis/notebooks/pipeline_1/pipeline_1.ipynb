{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Pipeline #1\n",
    "*Refer to `notebooks/README.md` for an explanation of the various pipelines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:31.354913Z",
     "start_time": "2022-04-07T22:47:31.043798Z"
    }
   },
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "\n",
    "# Importing datasets\n",
    "import opendatasets as od\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "\n",
    "# Graphing/Visualizing\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:33.893035Z",
     "start_time": "2022-04-07T22:47:32.167203Z"
    }
   },
   "outputs": [],
   "source": [
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:37.066014Z",
     "start_time": "2022-04-07T22:47:35.084187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Prep nltk library\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-defined (a bit jank...)\n",
    "sys.path.append(\"../utils\") # Adds higher directory for ControlSignal\n",
    "from ControlSignal import ControlSignal, CONTROL_ACTIONS, CONTROL_FLAGS, processSignals\n",
    "from ConfigParser import validateConfig\n",
    "from Grapher import graphWordDist, graphTweetDist\n",
    "sys.path.remove('../utils')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `extract` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:47.327270Z",
     "start_time": "2022-04-07T22:47:47.295179Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract(import_path, import_dest, **kwargs):\n",
    "    signals=[]\n",
    "    if kwargs.get('new_dataset'): \n",
    "        # Requesting new data\n",
    "        dest_dir = os.path.dirname(import_dest)\n",
    "        \n",
    "        # Check for existing dataset\n",
    "        if os.path.isfile(import_dest):\n",
    "            print('Found existing file:', import_dest)\n",
    "            user_input = input('Remove? (y/n)')\n",
    "            if user_input != 'y': # add a version suffix to newly imported file\n",
    "                existing_files = [f for f in os.listdir(dest_dir) if f.endswith('.csv')]\n",
    "                parts = import_dest.split('/')\n",
    "                ext_index = parts[-1].index('.')\n",
    "                parts[-1] = f\"{parts[-1][:ext_index]}_({len(existing_files)}){parts[-1][ext_index:]}\"\n",
    "                import_dest = os.path.join(*(parts))\n",
    "            else:\n",
    "                try:\n",
    "                    os.remove(import_dest)\n",
    "                except OSError as e:\n",
    "                    signals.append(ControlSignal(CONTROL_ACTIONS.WARNING, CONTROL_FLAGS.INVALID_LOCATION, f'Could not delete existing file. Received error {str(e)}'))\n",
    "\n",
    "        # Download dataset\n",
    "        od.download(import_path, data_dir=dest_dir)\n",
    "\n",
    "        # Collect downloaded dataset\n",
    "        ## ASSUMING KAGGLE DOWNLOAD!!\n",
    "        dataset_id = od.utils.kaggle_direct.get_kaggle_dataset_id(import_path) # in form 'username/dataset_name'\n",
    "        data_import_dir = os.path.join(dest_dir, dataset_id.split('/')[1]) \n",
    "\n",
    "        imported_filename = next(f for f in os.listdir(data_import_dir) if f.endswith('.csv'))\n",
    "\n",
    "        if not imported_filename:\n",
    "            print('Error importing data. File was either not downloaded or moved')\n",
    "            signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.INVALID_LOCATION, f'Failed importing data. File was either moved or not downloaded. Searched for .csv file in {data_import_dir}'))\n",
    "            return signals, None\n",
    "\n",
    "        # Move and rename file\n",
    "        os.rename(os.path.join(data_import_dir, imported_filename), import_dest)\n",
    "\n",
    "        # Remove temporary directory created when downloaded\n",
    "        try:\n",
    "            if os.listdir((data_import_dir)):\n",
    "                signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.INVALID_LOCATION, f'Import directory [{data_import_dir}] already exists! Unable to move new data'))\n",
    "                return signals, None\n",
    "            os.rmdir(data_import_dir)\n",
    "        except OSError as e:\n",
    "            print('Could not delete import directory, got' + str(e))\n",
    "            signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.INVALID_LOCATION, f'Could not delete import directory [{data_import_dir}]. Exiting for safety.'))\n",
    "            return signals, None\n",
    "\n",
    "\n",
    "    # Import data\n",
    "    raw_tweets_df = pd.read_csv(import_dest)\n",
    "\n",
    "    # Look for existing models\n",
    "    existing_models = {}\n",
    "    if (word_vec_path := kwargs.get('word_vec')):\n",
    "        existing_models['word_vec'] = Word2Vec.load(word_vec_path).wv\n",
    "    if (kmeans_path := kwargs.get('kmeans')):\n",
    "        existing_models['kmeans'] = load(kmeans_path)\n",
    "    if (embeddings_path := kwargs.get('embeddings')):\n",
    "        existing_models['embeddings'] = pd.read_csv(embeddings_path)\n",
    "    if (vectorizer_path := kwargs.get('vectorizer')):\n",
    "        existing_models['vectorizer'] = load(vectorizer_path)\n",
    "    if (linear_svc_path := kwargs.get('linear_svc')):\n",
    "        existing_models['linear_svc'] = (load(linear_svc_path), None) # Place holder for performance metrics\n",
    "    if (multi_nb_path := kwargs.get('multi_nb')):\n",
    "        existing_models['multi_nb'] = (load(multi_nb_path), None) # Place holder for performance metrics\n",
    "\n",
    "    return signals, (raw_tweets_df, existing_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper function for `transform`\n",
    "Handles user input for cluster sentiment assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need user input to determine each cluster's sentiment ##\n",
    "\n",
    "def setClusterSentiment(vectors, model, mapping, signals=[], display_terms=20):\n",
    "    print(f'** Top {display_terms} Similar Word Vectors By Cluster **\\n')\n",
    "\n",
    "    def collectSamples(multiplier=0):\n",
    "        word_vec_list = [vectors.similar_by_vector(model.cluster_centers_[x], \n",
    "                                                            topn=(display_terms * (multiplier+1)), \n",
    "                                                            restrict_vocab=None) for x in range(len(mapping))]\n",
    "        \n",
    "        cluster_values = np.array(list(zip(*[x[(display_terms * multiplier):] for x in word_vec_list])))\n",
    "\n",
    "        # Collect terms spanning multiple clusters for deciphering\n",
    "        term_freq, counts = np.unique([x[0] for x in np.vstack(cluster_values)], axis=0, return_counts=True)\n",
    "        unique_terms = term_freq[counts == 1]\n",
    "\n",
    "        # Separate unique from duplicate terms\n",
    "        uniq_cluster_vals = defaultdict(lambda : np.full(len(cluster_values), np.nan, dtype=object))\n",
    "        shared_cluster_vals = defaultdict(lambda : [0] * len(mapping))\n",
    "        for iy, ix in np.ndindex(cluster_values.shape[:len(mapping)-1]):\n",
    "            tmp = cluster_values[iy, ix]\n",
    "            if tmp[0] in unique_terms:\n",
    "                uniq_cluster_vals[ix][iy] = tuple(tmp)\n",
    "            else:\n",
    "                shared_cluster_vals[tmp[0]][ix] = tmp[1]\n",
    "\n",
    "        max_uniq_in_cluster = max([len([x for x in l if not pd.isnull(x)]) for l in uniq_cluster_vals.values()])\n",
    "        formatted_unique = np.array([np.pad(vals[~pd.isnull(vals)], \n",
    "                                        (0,max_uniq_in_cluster-np.count_nonzero(~pd.isnull(vals))), constant_values=None) \n",
    "                                        for vals in uniq_cluster_vals.values()], dtype=object).T\n",
    "\n",
    "        cols = [f'Cluster {x}' for x in range(len(mapping))]\n",
    "\n",
    "        print('Unique Terms from Clusters')\n",
    "        unique_terms_df = pd.DataFrame([[x[0] if x else '' for x in y] for y in formatted_unique], columns=cols)\n",
    "        display(unique_terms_df)\n",
    "\n",
    "        print('\\nDuplicate Terms from Clusters')\n",
    "        duplicate_terms_df = pd.DataFrame.from_dict(shared_cluster_vals, orient='index', columns=cols)\n",
    "        display(duplicate_terms_df)\n",
    "    \n",
    "    collectSamples()\n",
    "\n",
    "    ## Get input\n",
    "    \n",
    "    print('\\nLabel each cluster: -1 = negative, 0 = neutral, 1 = positive (\"r\" for new samples, \"q\" to exit)')\n",
    "    cluster_sentiment_defs = []\n",
    "    user_input = ''\n",
    "    resets = 0\n",
    "    while len(cluster_sentiment_defs) < len(mapping)-1 and user_input != 'q':\n",
    "        \n",
    "        user_input = input(f'Cluster {len(cluster_sentiment_defs)} value:')\n",
    "        if user_input == 'q':\n",
    "            signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.USER_INPUT))\n",
    "            return\n",
    "        if user_input == 'r':\n",
    "            print(f'\\n\\nGenerating next {display_terms} samples...\\n')\n",
    "            resets += 1\n",
    "            collectSamples(resets)\n",
    "            print('Current state:', cluster_sentiment_defs)\n",
    "            print('Setting cluster:', len(cluster_sentiment_defs))\n",
    "            continue\n",
    "        try:\n",
    "            value = int(user_input)\n",
    "            if value in cluster_sentiment_defs or value not in range(-1, 2):\n",
    "                print('Already used this sentiment or not in range (-1, 0, 1)')\n",
    "                continue\n",
    "            cluster_sentiment_defs.append(value)\n",
    "            print(f'Set cluster {len(cluster_sentiment_defs)-1} to {value} ({mapping[value]})')\n",
    "        except ValueError:\n",
    "            print('Need a number in range [-1, 0, 1]. Press q to exit')\n",
    "            \n",
    "    cluster_sentiment_defs.append((set(mapping.keys()) - set(cluster_sentiment_defs)).pop())\n",
    "\n",
    "    print(f'Set cluster {len(cluster_sentiment_defs)-1} to {cluster_sentiment_defs[-1]} ({mapping[cluster_sentiment_defs[-1]]})')\n",
    "    return cluster_sentiment_defs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `transform` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndFilter(raw_tweets_df, column_mappings={}, filter_words={}, **kwargs):\n",
    "    # Rename columns\n",
    "    tweets_df = raw_tweets_df[list(column_mappings.keys())].rename(columns=column_mappings) \\\n",
    "                    if column_mappings else raw_tweets_df.copy()\n",
    "\n",
    "    # Drop duplicate tweets\n",
    "    tweets_df = tweets_df.drop_duplicates(subset='tweet', keep='first')\n",
    "\n",
    "    # Initialize Lemmatizer\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    tweet_regexs = kwargs.get('cleen_tweet', [r'https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*',\n",
    "                                                r'\\$[a-zA-Z0-9]*',\n",
    "                                                r'[^a-zA-Z\\']',\n",
    "                                                r'\\@[a-zA-Z0-9]*'])\n",
    "    def cleanTweet(tweet):\n",
    "        tweet = tweet.lower()\n",
    "        for pattern in tweet_regexs:\n",
    "            tweet = re.sub(pattern, ' ', tweet)\n",
    "        tweet = ' '.join([w for w in tweet.split() if len(w) > 1])\n",
    "        \n",
    "        trimmed_lemma_words = [lemma.lemmatize(x) for x in nltk.wordpunct_tokenize(tweet) \n",
    "                                    if x not in stop_words]\n",
    "        clean_tweet = ' '.join(trimmed_lemma_words)\n",
    "        \n",
    "        return [lemma.lemmatize(x, nltk.corpus.reader.wordnet.VERB) \n",
    "                    for x in nltk.wordpunct_tokenize(clean_tweet) if x not in stop_words]\n",
    "\n",
    "    hashtag_regexs = kwargs.get('clean_hashtag', [r'\\$[a-zA-Z0-9]*', r'[^a-zA-Z\\']'])\n",
    "    def cleanHashtags(hashtags):\n",
    "        if hashtags:\n",
    "            hashtags = hashtags.lower()\n",
    "            for pattern in hashtag_regexs:\n",
    "                hashtags = re.sub(pattern, ' ', hashtags)\n",
    "            hashtags = hashtags.strip() \n",
    "        return hashtags\n",
    "    \n",
    "    \n",
    "    # Clean tweets\n",
    "    tweets_df['clean_tweet_words'] = tweets_df['tweet'].apply(lambda x: cleanTweet(x))\n",
    "    tweets_df['clean_tweet'] = tweets_df['clean_tweet_words'].apply(lambda x:' '.join(x))\n",
    "\n",
    "    # Clean hashtags\n",
    "    tweets_df[\"hashtags\"] = tweets_df[\"hashtags\"].astype(str)\n",
    "    tweets_df[\"hashtags\"] = tweets_df[\"hashtags\"].apply(lambda x: cleanHashtags(x))\n",
    "\n",
    "    # Convert date to datetime and extract month/year\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date'])\n",
    "    tweets_df['day'] = tweets_df['date'].dt.day\n",
    "    tweets_df['month'] = tweets_df['date'].dt.month\n",
    "    # tweets_df['year'] = tweets_df['date'].dt.year\n",
    "\n",
    "    if filter_words:\n",
    "        # Remove all tweets which do not have the provided target words\n",
    "        keywords_str = '|'.join(filter_words)\n",
    "        filtered_tweets_df = tweets_df.copy()\n",
    "        filtered_tweets_df = filtered_tweets_df[filtered_tweets_df[\"clean_tweet\"].str.contains(keywords_str)]\n",
    "        return tweets_df, filtered_tweets_df\n",
    "        \n",
    "    return tweets_df, pd.DataFrame([]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVectors(tweets_df, progress_per=50000, epohcs=30, **kwargs):\n",
    "    # Restructure the `clean_text` column\n",
    "    row_sentences = [row for row in tweets_df[\"clean_tweet_words\"]]\n",
    "\n",
    "    # Detect common phrases (bigrams) from a list of sentences\n",
    "    phrases = Phrases(row_sentences, min_count=1, progress_per=50000)\n",
    "    bigram = Phraser(phrases)\n",
    "    sentences = bigram[row_sentences]\n",
    "    \n",
    "    # Initialize vector model\n",
    "    if (word_vec_params := kwargs.get('word_vec_args')):\n",
    "        word_vec_model = Word2Vec(**word_vec_params)\n",
    "        word_vec_model.build_vocab(sentences, progress_per=progress_per)\n",
    "        word_vec_model.train(sentences, total_examples=word_vec_model.corpus_count, \n",
    "                                epochs=epohcs, report_delay=1)\n",
    "    else:\n",
    "        word_vec_model = Word2Vec(vector_size=300, \n",
    "                                window=5, \n",
    "                                min_count=4, \n",
    "                                workers=multiprocessing.cpu_count()-1,\n",
    "                                negative=20, \n",
    "                                sample=1e-5, \n",
    "                                alpha=0.03, \n",
    "                                min_alpha=0.007,  \n",
    "                                seed= 42)\n",
    "\n",
    "        # Establish dataset for the vector model\n",
    "        word_vec_model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "        # Train the model\n",
    "        word_vec_model.train(sentences, total_examples=word_vec_model.corpus_count, \n",
    "                                epochs=30, report_delay=1)\n",
    "    return word_vec_model.wv\n",
    "\n",
    "\n",
    "def buildWordEmbeddings(word_vectors, model, sentiment_defs, sentiment_map):\n",
    "     # Create a DataFrame of words with their embeddings and cluster values\n",
    "    words_df = pd.DataFrame(word_vectors.index_to_key)\n",
    "    words_df.columns = ['words']\n",
    "    words_df['vectors'] = words_df.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "    words_df['cluster'] = words_df.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "    words_df.cluster = words_df.cluster.apply(lambda x: x[0])\n",
    "\n",
    "    \n",
    "    words_df['cluster_value'] = [sentiment_defs[i] for i in words_df.cluster]\n",
    "\n",
    "    # Calculate proximity of words in each vector\n",
    "    calc_vector_nearness = lambda x: 1 / (model.transform([x.vectors]).min())\n",
    "    words_df['closeness_score'] = words_df.apply(calc_vector_nearness, axis=1)\n",
    "    words_df['sentiment_coeff'] = words_df.closeness_score * words_df.cluster_value\n",
    "\n",
    "    # Map sentiment encodings\n",
    "    words_df[\"sentiment\"] = words_df[\"cluster_value\"].map(sentiment_map)\n",
    "    return words_df\n",
    "\n",
    "def peekSentimentDistrib(tweets_df):\n",
    "    print('\\nCalculated Sentiment Distribution:')\n",
    "    display(tweets_df['sentiment'].value_counts())\n",
    "    user_input = input('Distribution okay? (y/n) ')\n",
    "    if user_input != 'y':\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:52.399831Z",
     "start_time": "2022-04-07T22:47:52.291560Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def transform(raw_tweets_df, sentiment_map, column_mappings={}, \n",
    "                                filter_words=[], existing_models={}, **kwargs):\n",
    "    signals = []\n",
    "    clean_tweets, filtered_tweets = cleanAndFilter(raw_tweets_df=raw_tweets_df, \n",
    "                                                    column_mappings=column_mappings, \n",
    "                                                    filter_words=filter_words, \n",
    "                                                    kwargs=kwargs)\n",
    "\n",
    "    if (existing_word_vec := existing_models.get('word_vec')):\n",
    "        word_vectors = existing_word_vec\n",
    "    else:\n",
    "        word_vectors = buildWordVectors(clean_tweets, kwargs)\n",
    "    \n",
    "    if (existing_kmeans := existing_models.get('kmeans')):\n",
    "        cluster_model = existing_kmeans\n",
    "    else:\n",
    "        # Build KMeans model to cluster words into positive, negative, and neutral clusters\n",
    "        if (kmeans_params := kwargs.get('kmeans_args')):\n",
    "            cluster_model = KMeans(**kmeans_params)\n",
    "        else:\n",
    "            cluster_model = KMeans(n_clusters=3, max_iter=1000, \n",
    "                                    random_state=42, n_init=50)\n",
    "        cluster_model = cluster_model.fit(X=word_vectors.vectors.astype('double'))\n",
    "                        \n",
    "    ############# Get Input ###############\n",
    "    cluster_sentiment_defs = setClusterSentiment(vectors=word_vectors, \n",
    "                                                    model=cluster_model, \n",
    "                                                    mapping=sentiment_map, \n",
    "                                                    signals=signals,\n",
    "                                                    display_terms=kwargs.get('display_terms'))\n",
    "    if not cluster_sentiment_defs:\n",
    "        return signals, None\n",
    "    print('\\nApplying sentiment mapping...')\n",
    "    #######################################\n",
    "\n",
    "    if (existing_embeddings := kwargs.get('embeddings')):\n",
    "        words_df = existing_embeddings\n",
    "    else:\n",
    "        words_df = buildWordEmbeddings(word_vectors=word_vectors, \n",
    "                                        model=cluster_model, \n",
    "                                        sentiment_defs=cluster_sentiment_defs, \n",
    "                                        sentiment_map=sentiment_map)\n",
    "        \n",
    "    # Get the sentiment for the entire tweet\n",
    "    threshold = kwargs.get('sentiment_threshold', 0.15)\n",
    "    words_cluster_dict = dict(zip(words_df.words, words_df.cluster_value))\n",
    "    def getSentiment(row):\n",
    "        total, count = 0, 0\n",
    "        test = row[\"clean_tweet_words\"]\n",
    "        for t in test:\n",
    "            total += int(words_cluster_dict.get(t, 0))\n",
    "            count += 1 \n",
    "            \n",
    "        avg = total / count\n",
    "        return -1 if (avg < -threshold) else 1 if (avg > threshold) else 0\n",
    "\n",
    "    # Add sentiment column (integer values)\n",
    "    filtered_tweets[\"sentiment_val\"] = filtered_tweets.apply(getSentiment, axis=1)\n",
    "    # Map integer sentiment to word value\n",
    "    filtered_tweets[\"sentiment\"] = filtered_tweets[\"sentiment_val\"].map(sentiment_map)\n",
    "\n",
    "    # Confirm sentiment spread with user\n",
    "    if not peekSentimentDistrib(filtered_tweets):\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.USER_INPUT, f'Distribution was unsatisfactory.'))\n",
    "        return signals, None\n",
    "    return signals, (filtered_tweets, words_df, cluster_sentiment_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, X_test, y_test):\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Build confusion matrix to evaluate the model results\n",
    "    confusion = confusion_matrix(y_test, y_pred, labels=np.unique(y_pred))\n",
    "\n",
    "    # Get classification report\n",
    "    classification = classification_report(y_test, y_pred, labels=np.unique(y_pred))\n",
    "\n",
    "    # Use score method to get accuracy of model\n",
    "    acc_score = model.score(X_test, y_test)\n",
    "\n",
    "    return {\n",
    "        'confusion': confusion,\n",
    "        'classification': classification,\n",
    "        'acc_score': acc_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:47:59.009834Z",
     "start_time": "2022-04-07T22:47:58.996891Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def model(sentiment_df, existing_models={}, **kwargs):\n",
    "    signals = []\n",
    "    # Convert each sentiment to df (no need to worry about memory crash, small dataset)\n",
    "    pos_df = sentiment_df[sentiment_df[\"sentiment\"]==\"positive\"]\n",
    "    neg_df = sentiment_df[sentiment_df[\"sentiment\"]==\"negative\"]\n",
    "    neu_df = sentiment_df[sentiment_df[\"sentiment\"]==\"neutral\"]\n",
    "\n",
    "    # Combine all sentiments in one df\n",
    "    sentiments_df_list = [pos_df, neg_df, neu_df] \n",
    "    agg_sentiment_df = pd.concat(sentiments_df_list)\n",
    "\n",
    "    # Split the data to training, testing, and validation data \n",
    "    test_size = kwargs.get('test_size', 0.2)\n",
    "    train_test_df, _ = train_test_split(agg_sentiment_df, test_size=test_size, random_state=10)\n",
    "\n",
    "    X = train_test_df['clean_tweet']\n",
    "    y = train_test_df['sentiment_val']\n",
    "\n",
    "    # Split the dataset set into training and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Instantiate TfidfVectorizer \n",
    "    if (existing_vectorizer := existing_models.get('vectorizer')):\n",
    "        vectorizer = existing_vectorizer\n",
    "    else: \n",
    "        if (vectorizer_params := kwargs.get('vectorizer_args')):\n",
    "            vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "        else:\n",
    "            vectorizer = TfidfVectorizer(min_df=3,\n",
    "                                            sublinear_tf=True,\n",
    "                                            ngram_range=(1,2),\n",
    "                                            stop_words='english')\n",
    "\n",
    "    # Fit vectorizer\n",
    "    X_train_tf = vectorizer.fit_transform(X_train.reset_index()[\"clean_tweet\"]).toarray()\n",
    "    X_test_tf = vectorizer.transform(X_test.reset_index()[\"clean_tweet\"]).toarray()\n",
    "\n",
    "    # Store features from the vectors\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a dict of Sentiment_val: sentiments to use with the confusion matrix\n",
    "    sentiment_id_df = agg_sentiment_df[['sentiment', 'sentiment_val']].drop_duplicates() \\\n",
    "                                                                        .sort_values('sentiment_val')\n",
    "    sentiment_to_id = dict(sentiment_id_df.values)\n",
    "\n",
    "    ## LinearSVC ##\n",
    "\n",
    "    if (existing_linear_svc := existing_models.get('linear_svc')):\n",
    "        # (NOTE: Must use same vectorizer from its fitting)\n",
    "        linearSVC = existing_linear_svc\n",
    "    else:\n",
    "        # Instantiate the model\n",
    "        linearSVC = LinearSVC()\n",
    "\n",
    "        # Fit the model\n",
    "        linearSVC.fit(X_train_tf, y_train)\n",
    "\n",
    "    svc_performance = testModel(model=linearSVC, X_test=X_test_tf, y_test=y_test)\n",
    "\n",
    "    ## MultinomialNB ##\n",
    "\n",
    "    if (existing_multi_nb := existing_models.get('multi_nb')):\n",
    "        # (NOTE: Must use same vectorizer from its fitting)\n",
    "        multiNB = existing_multi_nb\n",
    "    else:\n",
    "        # Instantiate the model\n",
    "        multiNB = MultinomialNB()\n",
    "\n",
    "        # Fit the model\n",
    "        multiNB.fit(X_train_tf, y_train)\n",
    "\n",
    "    nb_performance = testModel(model=multiNB, X_test=X_test_tf, y_test=y_test)\n",
    "\n",
    "    return signals, {\n",
    "                'linear_svc': (linearSVC, svc_performance),\n",
    "                'multi_nb': (multiNB, nb_performance),\n",
    "                'vectorizer': vectorizer,\n",
    "                'features': feature_names,\n",
    "                'sentiment_id': sentiment_id_df,\n",
    "                'sentiment_to': sentiment_to_id\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(transform_df, built_models={}, destinations={}, config=None):\n",
    "    signals = []\n",
    "    # Export the sentiment dataframe\n",
    "    if (transform_dest := destinations.get('transform')):\n",
    "        transform_df.to_csv(transform_dest)\n",
    "\n",
    "    if (config_loc := destinations.get('config')):\n",
    "        with open(config_loc, 'w') as f:\n",
    "            json.dump(config, f)\n",
    "    \n",
    "    # Pair up models with their export destination \n",
    "    model_exports = { name: path for name, path in destinations.items() \n",
    "                                            if (name in built_models and built_models[name]) }\n",
    "    for name, path in model_exports.items():\n",
    "        if name == 'word_vec':\n",
    "            built_models[name].save(path)\n",
    "        elif name == 'embeddings':\n",
    "            built_models[name].to_csv(path)\n",
    "        elif isinstance(built_models[name], tuple): # Check for model with associated performance metrics\n",
    "            dump(built_models[name][0], path)\n",
    "        else:\n",
    "            dump(built_models[name], path)\n",
    "            \n",
    "    # Save current notebook for import\n",
    "    if (notebook_dest := destinations.get('notebook')):\n",
    "        \n",
    "        !jupyter nbconvert --output {notebook_dest} --to script pipeline_1.ipynb\n",
    "\n",
    "        # Get rid of excess\n",
    "        with open(notebook_dest + '.py', 'r+') as fp:\n",
    "            lines = fp.readlines()\n",
    "            fp.seek(0)\n",
    "            fp.truncate()\n",
    "            cell_markers = set([])\n",
    "            term_index = len(lines) - 1\n",
    "            for i, line in enumerate(lines):\n",
    "                if '# Execute `pipeline`' in line:\n",
    "                    term_index = i\n",
    "                    break\n",
    "                elif '# In[' in line:\n",
    "                    cell_markers.add(i)\n",
    "\n",
    "            fp.writelines([l for i, l in enumerate(lines[:term_index]) if i not in cell_markers])\n",
    "    return signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `pipeline` function from above processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:48:17.303608Z",
     "start_time": "2022-04-07T22:48:17.298387Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def pipeline1(config_params, extract_args={}, transform_args={}, model_args={}, load_args={}):\n",
    "    # Parse arguments\n",
    "    signals, params = validateConfig(config=config_params, extract_config=extract_args, transform_config=transform_args, model_config=model_args, load_config=load_args)\n",
    "    processSignals(signals) # Process error/info signals\n",
    "    extract_params, transform_params, model_params, load_params = params\n",
    "\n",
    "    # Store run-specific information\n",
    "    execution_config = defaultdict(dict)\n",
    "\n",
    "    ## Extract (import)\n",
    "    print('Stage 1: Extracting...')\n",
    "    signals, result = extract(**extract_params)\n",
    "    processSignals(signals, load_params) # Process error/info signals\n",
    "    raw_tweets_df, imported_models = result\n",
    "    print('Completed Stage 1.', end='\\n\\n')\n",
    "\n",
    "    ## Transform\n",
    "    print('Stage 2: Transforming...')\n",
    "    signals, result = transform(raw_tweets_df=raw_tweets_df, existing_models=imported_models, **transform_params)\n",
    "    processSignals(signals, load_params) # Process error/info signals\n",
    "    tweet_sentiment_df, word_vecs, sentiment_defs = result\n",
    "\n",
    "    # Store sentiment encodings\n",
    "    execution_config['sentiment_vals'] = {\n",
    "        'value_mapping': transform_params['sentiment_map'],\n",
    "        'cluster_mapping': sentiment_defs\n",
    "    }\n",
    "    print('Completed Stage 2.', end='\\n\\n')\n",
    "\n",
    "    ## Modeling\n",
    "    if model_params.get('build_models'):\n",
    "        print('Stage 2.5: Modeling...')\n",
    "        signals, models = model(sentiment_df=tweet_sentiment_df, existing_models=imported_models, **model_params)\n",
    "        processSignals(signals, load_params) # Process error/info signals\n",
    "\n",
    "        imported_models.update(models) # Update previously imported models\n",
    "        print('Completed Stage 2.5.', end='\\n\\n')\n",
    "        \n",
    "    ## Loading (export)\n",
    "    print('Stage 3: Loading...')\n",
    "    signals = load(transform_df=tweet_sentiment_df, built_models=imported_models, destinations=load_params, config=execution_config)\n",
    "    processSignals(signals, load_params) # Process error/info signals\n",
    "    print('Completed Stage 3.', end='\\n\\n')\n",
    "    print('<done>')\n",
    "    \n",
    "    return tweet_sentiment_df, word_vecs, imported_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-07T22:52:33.738405Z",
     "start_time": "2022-04-07T22:50:56.370146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Could not find import parameter: [EXTRACT - word_vec]. Will not import this object.\n",
      "[INFO] Could not find import parameter: [EXTRACT - kmeans]. Will not import this object.\n",
      "[INFO] Could not find import parameter: [EXTRACT - embeddings]. Will not import this object.\n",
      "[INFO] Could not find import parameter: [EXTRACT - vectorizer]. Will not import this object.\n",
      "[INFO] Could not find import parameter: [EXTRACT - linear_svc]. Will not import this object.\n",
      "[INFO] Could not find import parameter: [EXTRACT - multi_nb]. Will not import this object.\n",
      "[INFO] Could not find optional parameter: [TRANSFORM - word_vec_args]. Will use default argument(s).\n",
      "[INFO] Could not find optional parameter: [TRANSFORM - kmeans_args]. Will use default argument(s).\n",
      "[INFO] Could not find optional parameter: [TRANSFORM - sentiment_threshold]. Will use default argument(s).\n",
      "[INFO] Could not find optional parameter: [TRANSFORM - sentiment_map]. Will use default argument(s).\n",
      "[INFO] Could not find optional parameter: [MODEL - vectorizer_args]. Will use default argument(s).\n",
      "[INFO] Could not find optional parameter: [MODEL - linear_svc_args]. Will use default argument(s).\n",
      "[INFO] Could not find optional parameter: [MODEL - multi_nb_args]. Will use default argument(s).\n",
      "Stage 1: Extracting...\n",
      "Completed Stage 1.\n",
      "\n",
      "Stage 2: Transforming...\n",
      "** Top 25 Similar Word Vectors By Cluster **\n",
      "\n",
      "Unique Terms from Clusters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster 0</th>\n",
       "      <th>Cluster 1</th>\n",
       "      <th>Cluster 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>westandwithukraine_zelensky</td>\n",
       "      <td>ball_steel</td>\n",
       "      <td>sarcasm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>review</td>\n",
       "      <td>resolve</td>\n",
       "      <td>ohio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>secret_weapon</td>\n",
       "      <td>interest</td>\n",
       "      <td>republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lukashenka</td>\n",
       "      <td>standingwithukraine</td>\n",
       "      <td>official_statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zelenskyyisawarhero</td>\n",
       "      <td>sweet</td>\n",
       "      <td>incredible_bravery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>natsechobbyist</td>\n",
       "      <td>piece_shit</td>\n",
       "      <td>beautiful_city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>world_leader</td>\n",
       "      <td>rem</td>\n",
       "      <td>westandwithukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>putinswar_putinwarcriminal</td>\n",
       "      <td>lesiavasylenko</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>repmtg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jackdetsch</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rblx_eddy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>holy_shit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>traffic</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>standwithukraine_ukrainianheroes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>close_airspace</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>west_ham</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>unbelievable_courage</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Cluster 0            Cluster 1           Cluster 2\n",
       "0        westandwithukraine_zelensky           ball_steel             sarcasm\n",
       "1                             review              resolve                ohio\n",
       "2                      secret_weapon             interest          republican\n",
       "3                         lukashenka  standingwithukraine  official_statement\n",
       "4                zelenskyyisawarhero                sweet  incredible_bravery\n",
       "5                     natsechobbyist           piece_shit      beautiful_city\n",
       "6                       world_leader                  rem  westandwithukraine\n",
       "7         putinswar_putinwarcriminal       lesiavasylenko                    \n",
       "8                             repmtg                                         \n",
       "9                         jackdetsch                                         \n",
       "10                         rblx_eddy                                         \n",
       "11                         holy_shit                                         \n",
       "12                           traffic                                         \n",
       "13  standwithukraine_ukrainianheroes                                         \n",
       "14                    close_airspace                                         \n",
       "15                          west_ham                                         \n",
       "16              unbelievable_courage                                         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Terms from Clusters\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster 0</th>\n",
       "      <th>Cluster 1</th>\n",
       "      <th>Cluster 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>guydorrellesq_morning</th>\n",
       "      <td>0.9998230338096619</td>\n",
       "      <td>0.9998050928115845</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>official</th>\n",
       "      <td>0.9998131394386292</td>\n",
       "      <td>0.99981290102005</td>\n",
       "      <td>0.9997695088386536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gotta_love</th>\n",
       "      <td>0.9998213648796082</td>\n",
       "      <td>0.9998109936714172</td>\n",
       "      <td>0.9997920393943787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good_news</th>\n",
       "      <td>0.9998147487640381</td>\n",
       "      <td>0.9998027682304382</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completely_agree</th>\n",
       "      <td>0.9998076558113098</td>\n",
       "      <td>0.9998083114624023</td>\n",
       "      <td>0.9997710585594177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scrap_metal</th>\n",
       "      <td>0.9998089075088501</td>\n",
       "      <td>0.999802827835083</td>\n",
       "      <td>0.9997905492782593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>standingwithukraine_stopputinnow</th>\n",
       "      <td>0.9998134970664978</td>\n",
       "      <td>0.9998030066490173</td>\n",
       "      <td>0.9997807741165161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remarkable</th>\n",
       "      <td>0.9998124837875366</td>\n",
       "      <td>0.9998079538345337</td>\n",
       "      <td>0.999774694442749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautiful_land</th>\n",
       "      <td>0.9998025894165039</td>\n",
       "      <td>0.9998051524162292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold</th>\n",
       "      <td>0.9998130798339844</td>\n",
       "      <td>0.9997991323471069</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gut_wrench</th>\n",
       "      <td>0.9998070597648621</td>\n",
       "      <td>0.9998050928115845</td>\n",
       "      <td>0.9997745752334595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inspire_world</th>\n",
       "      <td>0</td>\n",
       "      <td>0.9997984170913696</td>\n",
       "      <td>0.9997782707214355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>god_protect</th>\n",
       "      <td>0.9998117685317993</td>\n",
       "      <td>0.9998006224632263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tie</th>\n",
       "      <td>0.9998066425323486</td>\n",
       "      <td>0.9998049736022949</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>almost</th>\n",
       "      <td>0.9998083710670471</td>\n",
       "      <td>0.999798059463501</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohitsbenji</th>\n",
       "      <td>0.9998043179512024</td>\n",
       "      <td>0.9998019933700562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spanish</th>\n",
       "      <td>0.9998030066490173</td>\n",
       "      <td>0.999801516532898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heroesofukraine</th>\n",
       "      <td>0.9998041391372681</td>\n",
       "      <td>0.9997982978820801</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Cluster 0           Cluster 1  \\\n",
       "guydorrellesq_morning             0.9998230338096619  0.9998050928115845   \n",
       "official                          0.9998131394386292    0.99981290102005   \n",
       "gotta_love                        0.9998213648796082  0.9998109936714172   \n",
       "good_news                         0.9998147487640381  0.9998027682304382   \n",
       "completely_agree                  0.9998076558113098  0.9998083114624023   \n",
       "scrap_metal                       0.9998089075088501   0.999802827835083   \n",
       "standingwithukraine_stopputinnow  0.9998134970664978  0.9998030066490173   \n",
       "remarkable                        0.9998124837875366  0.9998079538345337   \n",
       "beautiful_land                    0.9998025894165039  0.9998051524162292   \n",
       "gold                              0.9998130798339844  0.9997991323471069   \n",
       "gut_wrench                        0.9998070597648621  0.9998050928115845   \n",
       "inspire_world                                      0  0.9997984170913696   \n",
       "god_protect                       0.9998117685317993  0.9998006224632263   \n",
       "tie                               0.9998066425323486  0.9998049736022949   \n",
       "almost                            0.9998083710670471   0.999798059463501   \n",
       "ohitsbenji                        0.9998043179512024  0.9998019933700562   \n",
       "spanish                           0.9998030066490173   0.999801516532898   \n",
       "heroesofukraine                   0.9998041391372681  0.9997982978820801   \n",
       "\n",
       "                                           Cluster 2  \n",
       "guydorrellesq_morning                              0  \n",
       "official                          0.9997695088386536  \n",
       "gotta_love                        0.9997920393943787  \n",
       "good_news                                          0  \n",
       "completely_agree                  0.9997710585594177  \n",
       "scrap_metal                       0.9997905492782593  \n",
       "standingwithukraine_stopputinnow  0.9997807741165161  \n",
       "remarkable                         0.999774694442749  \n",
       "beautiful_land                                     0  \n",
       "gold                                               0  \n",
       "gut_wrench                        0.9997745752334595  \n",
       "inspire_world                     0.9997782707214355  \n",
       "god_protect                                        0  \n",
       "tie                                                0  \n",
       "almost                                             0  \n",
       "ohitsbenji                                         0  \n",
       "spanish                                            0  \n",
       "heroesofukraine                                    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label each cluster: -1 = negative, 0 = neutral, 1 = positive (\"r\" for new samples, \"q\" to exit)\n",
      "Set cluster 0 to -1 (negative)\n",
      "Set cluster 1 to 0 (neutral)\n",
      "Set cluster 2 to 1 (positive)\n",
      "\n",
      "Applying sentiment mapping...\n",
      "\n",
      "Calculated Sentiment Distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "negative    15254\n",
       "neutral       595\n",
       "positive       56\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Stage 2.\n",
      "\n",
      "Stage 2.5: Modeling...\n",
      "Completed Stage 2.5.\n",
      "\n",
      "Stage 3: Loading...\n",
      "[NbConvertApp] WARNING | Config option `template_path` not recognized by `ScriptExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?\n",
      "[NbConvertApp] Converting notebook pipeline_1.ipynb to script\n",
      "[NbConvertApp] WARNING | Config option `template_path` not recognized by `PythonExporter`.  Did you mean one of: `extra_template_paths, template_name, template_paths`?\n",
      "[NbConvertApp] Writing 26063 bytes to ../pipeline_2/pipeline_1.py\n",
      "Completed Stage 3.\n",
      "\n",
      "<done>\n"
     ]
    }
   ],
   "source": [
    "USER_CONFIG_PATH = './config/config.json'\n",
    "sentiment_df, word_vecs, models = pipeline1(config_params=USER_CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pie chart of word sentiment distribution\n",
    "graphWordDist(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pie chart of tweet sentiment distribution\n",
    "graphTweetDist(sentiment_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d883baf2d2c020187d16fb74e1bc85e676b385dd78044a08a209b4abcafece"
  },
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
