{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis Pipeline #2\n",
    "*Refer to `notebooks/README.md` for an explanation of the various pipelines*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from functools import reduce \n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load, dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/petergish/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../pipeline_1\")\n",
    "from modules.pipeline_1 import pipeline1\n",
    "sys.path.remove(\"../pipeline_1\")\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from import_data import importData\n",
    "from config_parser import buildConfig, mergeDicts\n",
    "from control_signal import ControlSignal, CONTROL_ACTIONS, CONTROL_FLAGS, processSignals\n",
    "from grapher import Grapher\n",
    "sys.path.remove(\"../utils\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `extract` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DEF_TYPES = [str, LinearSVC, MultinomialNB]\n",
    "MODEL_NUM_FEATURES = [\n",
    "        ('n_features_in_', lambda x: x.n_features_in_)\n",
    "    ]\n",
    "\n",
    "def extract(sentiment_dataset=None, slava_vectorizer=None, slava_models=None, \n",
    "                x_col='', y_col='', slava_config=None, **kwargs):\n",
    "    signals = []\n",
    "    existing_models = {}\n",
    "\n",
    "    if not sentiment_dataset:\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.WARNING, CONTROL_FLAGS.INVALID_REQUIRED,\n",
    "                            'Missing sentiment dataset definition.'))\n",
    "    if not slava_vectorizer:\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.WARNING, CONTROL_FLAGS.INVALID_REQUIRED,\n",
    "                            'Missing the slava vectorizer definition produced by Pipeline 1.'))\n",
    "    if not slava_models:\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.WARNING, CONTROL_FLAGS.INVALID_REQUIRED,\n",
    "                            'Missing the slava prediction models definition(s) produced by Pipeline 1.'))\n",
    "    if signals:\n",
    "        return signals, None\n",
    "    if not x_col:\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.INVALID_REQUIRED,\n",
    "                        'Missing feature column definition for the sentiment dataset.'))\n",
    "    if not x_col:\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.INVALID_REQUIRED,\n",
    "                        'Missing label column definition for the sentiment dataset.'))\n",
    "    if signals:\n",
    "        return signals, None \n",
    "    \n",
    "\n",
    "    INVALID, VALID = 0, 1\n",
    "    searchParams = lambda keys, param: reduce(lambda d, k: d.get(k) , keys, param)\n",
    "    def validateObject(obj, allowed_types, expected_type=None, obj_protocol=None, required=False, err_signal=None, **kwargs): \n",
    "        if not hasattr(allowed_types, '__iter__') or isinstance(allowed_types, str) or allowed_types == str:\n",
    "            obj_type = allowed_types if isinstance(obj, allowed_types) else None\n",
    "        else:\n",
    "            obj_type = next((t for t in allowed_types if isinstance(obj, t)), None)\n",
    "        \n",
    "        if not obj_type:\n",
    "            if err_signal:\n",
    "                signals.append(err_signal)\n",
    "            return INVALID, None\n",
    "        \n",
    "        obj_signals, extracted_obj = importData(import_loc=obj, import_protocol=obj_protocol,\n",
    "                                        signals=signals, expected_type=expected_type, required=required, kwargs=kwargs)\n",
    "        signals.extend(obj_signals)\n",
    "        \n",
    "        if extracted_obj is None or (hasattr(extracted_obj,'size') and extracted_obj.size < 1):\n",
    "            return INVALID, None\n",
    "        \n",
    "        return VALID, extracted_obj\n",
    "        \n",
    "    \n",
    "    # Load sentiment df\n",
    "    valid_obj, sentiment_df = validateObject(obj=sentiment_dataset, obj_protocol=searchParams(['sentiment_dataset_protocol'], kwargs), \n",
    "                                    allowed_types=str, expected_type=pd.DataFrame, \n",
    "                                    err_signal=ControlSignal(CONTROL_ACTIONS.WARNING, CONTROL_FLAGS.MISSING_REQUIRED,\n",
    "                                        \"Missing ground-truth sentiment dataframe which is required.\"),\n",
    "                                    kwargs=kwargs)\n",
    "    if valid_obj == INVALID:\n",
    "        return signals, None\n",
    "\n",
    "    # Load vectorizer\n",
    "    valid_obj, vectorizer = validateObject(obj=slava_vectorizer, obj_protocol=searchParams(['slava_vectorizer_protocol'], kwargs),\n",
    "                                    allowed_types=str, expected_types=TfidfVectorizer, \n",
    "                                    err_signal=ControlSignal(CONTROL_ACTIONS.WARNING, CONTROL_FLAGS.MISSING_NECCESSARY, \n",
    "                                        'Must provide an existing vectorizer model associated with the sentiment data.'),\n",
    "                                    kwargs=kwargs)\n",
    "    if valid_obj == INVALID:\n",
    "        return signals, None\n",
    "        \n",
    "    num_vectorizer_features = len(vectorizer.idf_)    \n",
    "\n",
    "\n",
    "    # Load Pipeline 1 config\n",
    "    slava_config_data = {}\n",
    "    if isinstance(slava_config, str):\n",
    "        try:\n",
    "            with(open(slava_config, 'r') as f):\n",
    "                slava_config_data = json.load(f)\n",
    "        except:\n",
    "            signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.IMPORT_EXCEPTION,\n",
    "                            f'Could not read Pipeline 1 configuration from {slava_config}'))\n",
    "            return signals, None\n",
    "    elif not isinstance(slava_config, dict):\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.MISSING_NECCESSARY,\n",
    "                            (f'Invalid argument type for `slava_config`: {type(slava_config)} - {slava_config}\\n' + \n",
    "                            'Expected str or dict')))\n",
    "        return signals, None\n",
    "    else:\n",
    "        slava_config_data = slava_config\n",
    "    \n",
    "    if not (model_config := slava_config_data.get('MODEL')) or not model_config.get('sentiment_vals'):\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.MISSING_REQUIRED,\n",
    "                        f\"Provided `slava_config` is missing required configuration - [MODEL][sentiment_vals]\"))\n",
    "        return signals, None\n",
    "            \n",
    "    slava_sentiment_vals = slava_config_data['MODEL']['sentiment_vals']\n",
    "    \n",
    "\n",
    "    def validateModel(model_name, model_obj):\n",
    "        if not model_obj:\n",
    "            return False\n",
    "        features_accessor = next((x for x in MODEL_NUM_FEATURES if hasattr(model_obj, x[0])), None)\n",
    "        if not features_accessor:\n",
    "            return False\n",
    "        num_model_features = features_accessor[1](model_obj)\n",
    "        if num_model_features != num_vectorizer_features: # Model must be associated with the vectorizer!\n",
    "            signals.append(ControlSignal(CONTROL_ACTIONS.WARNING, CONTROL_FLAGS.FILE_MANAGEMENT,\n",
    "                                            (f'Mismatched number of features between model [{model_name}]' +\n",
    "                                                f'and the provided vectorizer.\\n# Model Features: {num_model_features}\\n' +\n",
    "                                                f'# Vectorizer Features: {num_vectorizer_features}')))\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "\n",
    "    if isinstance(slava_models, (tuple, list)):\n",
    "        for model_info in slava_models:\n",
    "            if isinstance(model_info, (tuple, list)):\n",
    "                model_name, model_def = model_info[0], model_info[1]  \n",
    "            else:\n",
    "                model_name, model_def = model_info['name'], { k: v for k, v in model_info.items() if k != 'name' }\n",
    "            \n",
    "            if 'protocol' not in model_name:\n",
    "                valid_obj, model = validateObject(obj=model_def, obj_protocol=searchParams([f'{model_name}_protocol'], slava_models), \n",
    "                                        allowed_types=MODEL_DEF_TYPES, \n",
    "                                        err_signal=ControlSignal(CONTROL_ACTIONS.INFO, CONTROL_FLAGS.IMPORT_EXCEPTION,\n",
    "                                            f'Could not import predictive model: {model_def}'))\n",
    "                if valid_obj == INVALID:\n",
    "                    continue\n",
    "                if validateModel(model_name, model):\n",
    "                    existing_models[model_name] = model\n",
    "\n",
    "    elif isinstance(slava_models, dict):\n",
    "        for model_name, model_def in slava_models.items():\n",
    "            if 'protocol' not in model_name:\n",
    "                valid_obj, model = validateObject(obj=model_def, obj_protocol=searchParams([f'{model_name}_protocol'], slava_models), \n",
    "                                        allowed_types=MODEL_DEF_TYPES, \n",
    "                                        err_signal=ControlSignal(CONTROL_ACTIONS.INFO, CONTROL_FLAGS.IMPORT_EXCEPTION,\n",
    "                                            f'Could not import predictive model: {model_def}'))\n",
    "                if valid_obj == INVALID:\n",
    "                    continue\n",
    "                if validateModel(model_name, model):\n",
    "                    existing_models[model_name] = model\n",
    "    \n",
    "    if not existing_models:\n",
    "        signals.append(ControlSignal(CONTROL_ACTIONS.ABORT, CONTROL_FLAGS.MISSING_REQUIRED),\n",
    "                        'Could not import Slava models which are required for inferences.')\n",
    "        return signals, None\n",
    "\n",
    "    X = sentiment_df[x_col]\n",
    "    y = sentiment_df[y_col]\n",
    "\n",
    "\n",
    "    # Transform text using vectorizer\n",
    "    X_test = vectorizer.transform(X.reset_index()[x_col]).toarray()\n",
    "    X_test_fit = vectorizer.fit_transform(X.reset_index()[x_col]).toarray()\n",
    "\n",
    "    # Collect features\n",
    "    feature_names = vectorizer.get_feature_names_out() \n",
    "\n",
    "    # Load linearSVC\n",
    "    if (linear_svc_path := kwargs.get('linear_svc')):\n",
    "        linear_svc = load(linear_svc_path)\n",
    "        existing_models['linear_svc'] = (linear_svc, {}) # Place holder for performance metrics\n",
    "\n",
    "    # Load MultinomialNB\n",
    "    if (multi_nb_path := kwargs.get('multi_nb')):\n",
    "        existing_models['multi_nb'] = (load(multi_nb_path), {}) # Place holder for performance metrics\n",
    "\n",
    "    model_params = {\n",
    "        'x_test': X_test,\n",
    "        'x_test_fit': X_test_fit,\n",
    "        'y_test': y,\n",
    "        'features': feature_names\n",
    "    }\n",
    "    \n",
    "    return signals, (sentiment_df, existing_models, model_params, slava_sentiment_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(sentiment_df, existing_models, model_params, sentiment_vals, **kwargs):\n",
    "    signals = []\n",
    "    \n",
    "    # Map sentiment encodings\n",
    "    p1_sentiment_encoding = sentiment_vals['value_mapping']\n",
    "\n",
    "    sentiment_codes = np.array([int(x) for x in p1_sentiment_encoding.keys()])\n",
    "    sentiment_labels = np.array(list(p1_sentiment_encoding.values()))\n",
    "\n",
    "    map_offset = np.abs(sentiment_codes.min())\n",
    "    mapping_arr = np.zeros(len(sentiment_codes), dtype=sentiment_labels.dtype)\n",
    "    mapping_arr[sentiment_codes+map_offset] = sentiment_labels\n",
    "\n",
    "    model_predictions = {}\n",
    "    for model_name, model in existing_models.items():\n",
    "        X_test = model_params['x_test']\n",
    "\n",
    "        # Generate prediction\n",
    "        inferences = model.predict(X_test)\n",
    "        predictions_arr = mapping_arr[inferences]\n",
    "\n",
    "        # Build df from predictions\n",
    "        zipped_data = zip(sentiment_df['clean_tweet'], sentiment_df['sentiment_val'], predictions_arr)\n",
    "        inferences_df = pd.DataFrame(zipped_data, columns=['clean_tweet', 'pred_sentiment_val', 'pred_sentiment'])\n",
    "        \n",
    "        model_predictions[model_name] = inferences_df\n",
    "\n",
    "    return signals, (model_predictions, mapping_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `inspect` function\n",
    "Used to calculate each model's performance\n",
    "\n",
    "#### **Cluster** model validation\n",
    "1. Internal validation\n",
    "    - Typically will combine cohesion (within each cluster) and separation (between different clusters)\n",
    "    - Compute the validation score of each cluster and then uses weights in the aggregation to produce a final score for the entire model\n",
    "\n",
    "2. External validation\n",
    "    - Necessary to have *True* cluster labels\n",
    "    - Measure the statistical similarity between the *True* cluster labels and the actual values\n",
    "\n",
    "#### **Classification** metrics\n",
    "1. Classification Accuracy:\n",
    "    - The ratio of correct predictions to the total number of predicitions\n",
    "    - Popular but flawed (often misused/misinterpreted); there are two criteria to meet for this calculation:\n",
    "        1. Equal number of observations in all classes\n",
    "        2. All predictions and prediction errors are equally important\n",
    "2. Log Loss\n",
    "    - Evaluates the predictions of probabilities of membership to a given class\n",
    "    - Can be seen as a measure of confidence for a prediction algorithm\n",
    "3. Area Under ROC Curve\n",
    "    - Designed for binary classification problems\n",
    "4. Confusion Matrix\n",
    "    - Provides the accuracy of a model which has two or more classes\n",
    "    - Presents the predicitions in relation to the accuracy outcome\n",
    "5. Classificaiton Report\n",
    "    - `scikit-learn`'s function to summarize a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small evaluation functions to be used by `inspect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MetricReports:\n",
    "\n",
    "    def getTests():\n",
    "        return {\n",
    "            'cross_val': MetricReports.crossValidation,\n",
    "            'confusion': MetricReports.confusionMatrix,\n",
    "            'classification': MetricReports.classificationReport\n",
    "        }\n",
    "\n",
    "    def execute(model, features, y_test, y_pred, **kwargs):\n",
    "        return [\n",
    "            MetricReports.crossValidation(model, features, y_pred, **kwargs),\n",
    "            MetricReports.confusionMatrix(y_test, y_pred),\n",
    "            MetricReports.classificationReport(y_test, y_pred)\n",
    "        ]\n",
    "\n",
    "    ## Metric Functions ##\n",
    "\n",
    "    def crossValidation(model, features, y_pred, scoring=None, kfold=5):\n",
    "        res = []\n",
    "        if not scoring:\n",
    "            scoring = ['accuracy']\n",
    "        for score in scoring:\n",
    "            res.append({\n",
    "                'name': f'CV Classification - {score}',\n",
    "                'result': cross_val_score(estimator=model,\n",
    "                                            X=features, \n",
    "                                            y=y_pred, \n",
    "                                            scoring=score, \n",
    "                                            cv=kfold)\n",
    "            })\n",
    "        return res\n",
    "\n",
    "    def confusionMatrix(y_test, y_pred):\n",
    "        ''' FIXME: This might not be a valid metric... not sure if it can handle unlabeled data\n",
    "        '''\n",
    "        return {\n",
    "            'name': 'Confusion Matrix',\n",
    "            'result': metrics.confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "        }\n",
    "\n",
    "    def classificationReport(y_test, y_pred):\n",
    "        ''' FIXME: This might not be a valid metric... not sure if it can handle unlabeled data\n",
    "        '''\n",
    "        return {\n",
    "            'name': 'Classification Report',\n",
    "            'result': metrics.classification_report(y_true=y_test, y_pred=y_pred, output_dict=True)\n",
    "        }\n",
    "\n",
    "\n",
    "## Best suited for unsupervised clustering algorithms ##\n",
    "\n",
    "class MetricScores:\n",
    "\n",
    "    def getTests():\n",
    "        return {\n",
    "            'silhouette': MetricScores.silhouetteScore,\n",
    "            'calinski_harabaz': MetricScores.calinskiHarabaz,\n",
    "            'dabies_bouldin': MetricScores.dabiesBouldin,\n",
    "            'mean_acc': MetricScores.meanAccuracy\n",
    "        }\n",
    "\n",
    "    def execute(model, features, y_pred):\n",
    "        return [\n",
    "            MetricScores.silhouetteScore(features, y_pred),\n",
    "            MetricScores.calinskiHarabaz(features, y_pred),\n",
    "            MetricScores.dabiesBouldin(features, y_pred),\n",
    "            MetricScores.meanAccuracy(model, features, y_pred)\n",
    "        ]\n",
    "\n",
    "    ## Metric Functions ##\n",
    "\n",
    "    def silhouetteScore(features, y_pred):\n",
    "        ''' Attempts to describe how similar a datapoint is to other datapoints in its cluster, \n",
    "        relative to datapoints not in its cluster (aggregated over all datapoints to get the score for \n",
    "        an overall clustering). It evaluates how ‘distinct’ the clusters are in space\n",
    "        It's bounded between -1 and 1. Closer to -1 suggests incorrect clustering, while \n",
    "        closer to +1 shows that each cluster is very dense.\n",
    "        '''\n",
    "        return {\n",
    "            'name': 'Silhouette Score',\n",
    "            'result': metrics.silhouette_score(X=features, labels=y_pred)\n",
    "        }\n",
    "\n",
    "    def calinskiHarabaz(features, y_pred):\n",
    "        ''' A ratio of the variance of a datapoint compared to points in other clusters, \n",
    "        against the variance compared to points within its cluster. This score is not bounded.\n",
    "        '''\n",
    "        return {\n",
    "            'name': 'Calinski Harabaz Index',\n",
    "            'result': metrics.calinski_harabasz_score(X=features, labels=y_pred)\n",
    "        }\n",
    "\n",
    "    def dabiesBouldin(features, y_pred):\n",
    "        ''' The average similarity measure of each cluster with its most similar cluster, \n",
    "        where similarity is the ratio of within-cluster distances to between-cluster distances. \n",
    "        Thus, clusters which are farther apart and less dispersed will result in a better score.\n",
    "        The minimum score is zero, with lower values indicating better clustering.\n",
    "        '''\n",
    "        return {\n",
    "            'name': 'Davies-Bouldin Index',\n",
    "            'result': metrics.davies_bouldin_score(X=features, labels=y_pred)\n",
    "        }\n",
    "\n",
    "    def meanAccuracy(model, features, y_pred):\n",
    "        '''\n",
    "        '''\n",
    "        return {\n",
    "            'name': 'Mean Accuracy',\n",
    "            'result': model.score(X=features, y=y_pred)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `evaluate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(sentiment_df, prediction_dfs, existing_models, model_params, **kwargs):\n",
    "    signals = []\n",
    "\n",
    "    # Set up variables\n",
    "    cv_scores = kwargs.get('cv_scores', ['accuracy'])\n",
    "    # crossVal_accuracies = list(zip(cv_scores, [[]]*len(cv_scores)))\n",
    "    crossVal_accuracies = defaultdict(list)\n",
    "\n",
    "    test_names = list(MetricReports.getTests().keys()) + list(MetricScores.getTests().keys())\n",
    "    metric_results = { m: { k: [] for k in test_names } for m in existing_models.keys() }\n",
    "    cv_dfs = { m: {} for m in existing_models.keys() }\n",
    "\n",
    "     # y_test = tweets_df['sentiment_val']\n",
    "    y_test = model_params['y_test']\n",
    "    X_test = model_params['x_test']\n",
    "    X_test_fit = model_params['x_test_fit']\n",
    "\n",
    "    ## Begin evaluations\n",
    "    for model_name, model in existing_models.items():\n",
    "        y_pred = prediction_dfs[model_name]['pred_sentiment_val']\n",
    "\n",
    "        metric_reports = MetricReports.execute(model=model, \n",
    "                                                features=X_test, \n",
    "                                                y_test=y_test, \n",
    "                                                y_pred=y_pred, \n",
    "                                                scoring=cv_scores, \n",
    "                                                kfold=kwargs.get('kfold'))\n",
    "        metric_scores = MetricScores.execute(model=model, \n",
    "                                                features=X_test, \n",
    "                                                y_pred=y_pred)\n",
    "\n",
    "        metric_results[model_name] = metric_reports + metric_scores\n",
    "\n",
    "        # Aggregate cross validation results\n",
    "        for results in metric_reports:\n",
    "            iterable = [results] if not isinstance(results, (list, tuple)) else results\n",
    "            for res in iterable:\n",
    "                if 'CV' in res['name']:\n",
    "                    score_name = res['name'].split(' ')[-1]\n",
    "                    indexed_acc = [(model_name, idx, acc) for idx, acc in enumerate(res['result'])]\n",
    "                    crossVal_df = pd.DataFrame(indexed_acc, columns=['model_name', 'fold_idx', score_name])\n",
    "                    crossVal_accuracies[score_name].append(crossVal_df)\n",
    "        \n",
    "        # Combine cross val results by scoring type\n",
    "        for score_type, models in crossVal_accuracies.items():\n",
    "            cv_dfs[model_name][score_type] = pd.concat(models)\n",
    "\n",
    "    # Create a dict of Sentiment_val\n",
    "    sentiment_id_df = sentiment_df[['sentiment_val', 'sentiment']].drop_duplicates().sort_values('sentiment_val')\n",
    "    sentiment_to_id = dict(sentiment_id_df.values)\n",
    "\n",
    "    sentiment_maps = {\n",
    "        'sentiment_id': sentiment_id_df.to_json(),\n",
    "        'sentiment_to': sentiment_to_id\n",
    "    }\n",
    "\n",
    "    return signals, (metric_results, cv_dfs, sentiment_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `load` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(prediction_dfs, results={}, cv_dfs={}, destinations={}, exec_config=None):\n",
    "    signals = []\n",
    "\n",
    "    # Export the execution config\n",
    "    if (config_loc := destinations.get('config')):\n",
    "        with open(config_loc, 'w') as f:\n",
    "            json.dump(exec_config, f)\n",
    "    \n",
    "    def loadGroup(group_name, group_data):\n",
    "        if not group_name in destinations:\n",
    "            return\n",
    "        group_destinations = destinations[group_name]\n",
    "        export_locs = { name: path for name, path in group_destinations.items()\n",
    "                                        if (name in group_data and group_data.get(name) is not None) }\n",
    "        for name, path in export_locs.items():\n",
    "            export_obj = group_data[name]\n",
    "            if isinstance(export_obj, pd.DataFrame): # Expected origin: `prediction_dfs`\n",
    "                export_obj.to_csv(path)\n",
    "            elif isinstance(export_obj, dict): # Expected origin: `cv_dfs`\n",
    "                aggregated_obj = pd.concat(export_obj.values(), keys=export_obj.keys())\n",
    "                aggregated_obj.to_csv(path)\n",
    "            else:\n",
    "                dump(export_obj, path) # Expected origin: `results` or `built_models`\n",
    "    \n",
    "    for name, data in zip(['predictions', 'metrics', 'cross_validations'], \n",
    "                            [prediction_dfs, results, cv_dfs]):\n",
    "        if data:\n",
    "            loadGroup(name, data)\n",
    "            \n",
    "    # Save current notebook for import\n",
    "    if (notebook_dest := destinations.get('notebook')):\n",
    "        \n",
    "        !jupyter nbconvert --output {notebook_dest} --to script pipeline_1.ipynb\n",
    "\n",
    "        # Get rid of excess\n",
    "        with open(notebook_dest + '.py', 'r+') as fp:\n",
    "            lines = fp.readlines()\n",
    "            fp.seek(0)\n",
    "            fp.truncate()\n",
    "            cell_markers = set([])\n",
    "            term_index = len(lines) - 1\n",
    "            for i, line in enumerate(lines):\n",
    "                if '# Execute `pipeline`' in line:\n",
    "                    term_index = i\n",
    "                    break\n",
    "                elif '# In[' in line:\n",
    "                    cell_markers.add(i)\n",
    "\n",
    "            fp.writelines([l for i, l in enumerate(lines[:term_index]) if i not in cell_markers])\n",
    "    return signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build `pipeline` function using above processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline2(default_configs, user_configs=None, extract_args={}, model_args={}, evaluate_args={}, load_args={}, log_level=None, **kwargs):\n",
    "\n",
    "    parsing_signals, valid_p1_params = buildConfig(dflt_configs=default_configs, usr_configs=user_configs,\n",
    "                                                    extract_config=extract_args, evaluate_config=evaluate_args,\n",
    "                                                    model_config=model_args, load_config=load_args,\n",
    "                                                    nested_params='PIPE_1')\n",
    "    processSignals(signals=parsing_signals, log_level=log_level) # Process error/info signals\n",
    "\n",
    "    p2_user_config = user_configs[0] if hasattr(user_configs, '__iter__') else user_configs\n",
    "    p2_dflt_config = default_configs[0] if hasattr(default_configs, '__iter__') else default_configs\n",
    "    parsing_signals, valid_p2_params = buildConfig(dflt_configs=p2_dflt_config, usr_configs=p2_user_config, \n",
    "                                                    extract_config=extract_args, evaluate_config=evaluate_args,\n",
    "                                                    model_config=model_args, load_config=load_args,\n",
    "                                                    excluded_params='PIPE_1')\n",
    "    processSignals(signals=parsing_signals, log_level=LOG_LEVEL)\n",
    "\n",
    "    pipeline_stages = ['EXTRACT', 'MODEL', 'EVALUATE', 'LOAD']\n",
    "    p2_extract_params, p2_model_params, p2_evaluate_params, p2_load_params = itemgetter(*pipeline_stages)(valid_p2_params)\n",
    "\n",
    "    # Store run-specific information\n",
    "    execution_config = defaultdict(dict)\n",
    "    print('\\n--- Executing Pipeline 2 ---\\n')\n",
    "\n",
    "    ## Extract (import)\n",
    "    print('[Pipeline 2] Stage 1: Extracting...')\n",
    "    extract_signals, extracted_data = extract(**p2_extract_params)\n",
    "    processSignals(signals=extract_signals, generated_files=p2_load_params, log_level=log_level) # Process error/info signals\n",
    "    \n",
    "    if not extracted_data:\n",
    "        print('\\n*** Aborting Pipeline 2 ***')\n",
    "        print('Will attempt to use Pipeline 1 to produce required data.\\n')\n",
    "\n",
    "        # Transform dataset using Pipeline 1 to produce a sentiment distribution of the data\n",
    "        sentiment_df, word_vecs, models = pipeline1(default_configs=valid_p1_params)\n",
    "        extract_signals, extracted_data = extract(sentiment_import=sentiment_df, vectorizer=models.get('vectorizer'), \n",
    "                                                    **{ k: v for k, v in p2_extract_params.items() if k != 'vectorizer' })\n",
    "        processSignals(signals=extract_signals, generated_files=p2_load_params, log_level=log_level) # Process error/info signals\n",
    "    \n",
    "    sentiment_df, existing_models, model_params, slava_sentiment_vals = extracted_data\n",
    "    print('[Pipeline 2] Completed Stage 1.', end='\\n\\n')\n",
    "\n",
    "    ## Model\n",
    "    print('[Pipeline 2] Stage 2: Modeling...')\n",
    "    model_signals, model_data = model(sentiment_df=sentiment_df, existing_models=existing_models,\n",
    "                                        model_params=model_params, sentiment_vals=slava_sentiment_vals,\n",
    "                                        **p2_model_params)\n",
    "    processSignals(signals=model_signals, generated_files=p2_load_params, log_level=log_level) # Process error/info signals\n",
    "    model_predictions, mapping_arr = model_data\n",
    "    execution_config['mapping_array'] = mapping_arr.tolist()\n",
    "    print('[Pipeline 2] Completed Stage 2.', end='\\n\\n')\n",
    "\n",
    "    ## Evaluation\n",
    "    print('[Pipeline 2] Stage 3: Evaluating...')\n",
    "    evaluation_signals, evaluation_data = evaluate(sentiment_df=sentiment_df, prediction_dfs=model_predictions,\n",
    "                                                    existing_models=existing_models, model_params=model_params,\n",
    "                                                    **p2_evaluate_params)\n",
    "    processSignals(signals=evaluation_signals, generated_files=p2_load_params, log_level=log_level) # Process error/info signals\n",
    "    metric_results, cross_val_dfs, sentiment_maps = evaluation_data\n",
    "    execution_config['sentiment_maps'] = sentiment_maps\n",
    "    print('[Pipeline 2] Completed Stage 3.', end='\\n\\n')\n",
    "    valid_p2_params['PIPE_1'] = valid_p1_params\n",
    "    mergeDicts(execution_config, valid_p2_params)\n",
    "\n",
    "    ## Loading (export)\n",
    "    print('[Pipeline 2] Stage 4: Loading...')\n",
    "    load_signals = load(prediction_dfs=model_predictions, results=metric_results, \n",
    "                        cv_dfs=cross_val_dfs, destinations=p2_load_params, exec_config=execution_config)\n",
    "    processSignals(signals=load_signals, generated_files=p2_load_params, log_level=log_level) # Process error/info signals\n",
    "    print('[Pipeline 2] Completed Stage 4.', end='\\n\\n')\n",
    "    print('[Pipeline 2] <done>')\n",
    "    \n",
    "    return model_predictions, metric_results, cross_val_dfs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Executing Pipeline 2 ---\n",
      "\n",
      "[Pipeline 2] Stage 1: Extracting...\n",
      "[Pipeline 2] Completed Stage 1.\n",
      "\n",
      "[Pipeline 2] Stage 2: Modeling...\n",
      "[Pipeline 2] Completed Stage 2.\n",
      "\n",
      "[Pipeline 2] Stage 3: Evaluating...\n",
      "[Pipeline 2] Completed Stage 3.\n",
      "\n",
      "[Pipeline 2] Stage 4: Loading...\n",
      "[Pipeline 2] Completed Stage 4.\n",
      "\n",
      "[Pipeline 2] <done>\n"
     ]
    }
   ],
   "source": [
    "PIPE1_USER_CONFIG = '../pipeline_1/config/user_config.json'\n",
    "PIPE2_USER_CONFIG = './config/user_config.json'\n",
    "\n",
    "PIPE1_DFLT_CONFIG = '../pipeline_1/config/default_config.json'\n",
    "PIPE2_DFLT_CONFIG= './config/default_config.json'\n",
    "LOG_LEVEL = CONTROL_ACTIONS.WARNING\n",
    "\n",
    "model_predictions, metric_results, cross_val_dfs = pipeline2(default_configs=[PIPE2_DFLT_CONFIG, PIPE1_DFLT_CONFIG],\n",
    "                                                            user_configs=[PIPE2_USER_CONFIG, PIPE1_USER_CONFIG],\n",
    "                                                            log_level=LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "get_scorer_names()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10d883baf2d2c020187d16fb74e1bc85e676b385dd78044a08a209b4abcafece"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
